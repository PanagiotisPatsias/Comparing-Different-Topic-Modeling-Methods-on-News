{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the best parameters for the `Newsgroup` dataset was challenging. We tested a wide range of values for the hyperparameters and experimented with several topic numbers. In the notebook below, youâ€™ll see the results for topics 7 and 10, which were among the best and yielded closely competitive outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "from gensim.models.ldamodel import LdaModel as LDA\n",
    "from sklearn.metrics import normalized_mutual_info_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fetch_7newsgroups.csv')\n",
    "df.dropna(subset=['label'],inplace = True, ignore_index= True)\n",
    "df['label'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Training and Test Sets \n",
    "### with a 5% Test Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.05, random_state=42)\n",
    "train['div'] = 'train'\n",
    "test['div'] = 'test'\n",
    "\n",
    "df_newsgroups_split = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"<URL>\", text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '<EMAIL>', text)\n",
    "    \n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    text = re.sub(r\"[!\\\"#$%&'()*+,/:;<=>?@[\\\\]^_`{|}~]\", \" \", text)\n",
    "    \n",
    "    # Tokenize and selectively remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_newsgroups_split['preprocessed_text'] = df_newsgroups_split['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newsgroups_split['tok'] = df_newsgroups_split['preprocessed_text'].apply(lambda x: set(x.split()))  \n",
    "train_docs = df_newsgroups_split[df_newsgroups_split['div']=='train']['tok'].to_numpy() \n",
    "dictionary = gensim.corpora.Dictionary(train_docs) \n",
    "\n",
    "df_newsgroups_split['corpus'] = [dictionary.doc2bow(doc) for doc in df_newsgroups_split['tok'].to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def q_metrics(y_true, y_pred1):\n",
    "    contigency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred1)\n",
    "    purity = np.sum(np.amax(contigency_matrix, axis=0)) / np.sum(contigency_matrix)\n",
    "    print('purity_score:',purity)\n",
    "    print('NMI:',metrics.normalized_mutual_info_score(y_true, y_pred1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Parameter Selection for Best Model Performance Topics 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_score: 0.42665173572228443\n",
      "NMI: 0.3006627233441655\n",
      "Coherence Score: 0.5310053515309129\n",
      "i: 10 ; alpha: symmetric ; beta: auto\n",
      "Training Time: 75.27 seconds, Response Time: 0.25 seconds, Coherence Score: 0.5310053515309129\n",
      "\n",
      "purity_score: 0.38633818589025753\n",
      "NMI: 0.3054491138005524\n",
      "Coherence Score: 0.5645498176533866\n",
      "i: 10 ; alpha: symmetric ; beta: 0.4\n",
      "Training Time: 64.84 seconds, Response Time: 0.18 seconds, Coherence Score: 0.5645498176533866\n",
      "\n",
      "purity_score: 0.24748040313549832\n",
      "NMI: 0.006474976925318856\n",
      "Coherence Score: 0.49704369545519406\n",
      "i: 10 ; alpha: symmetric ; beta: 0.7\n",
      "Training Time: 61.30 seconds, Response Time: 0.17 seconds, Coherence Score: 0.49704369545519406\n",
      "\n",
      "purity_score: 0.47928331466965285\n",
      "NMI: 0.3767308977816532\n",
      "Coherence Score: 0.4145923169148483\n",
      "i: 10 ; alpha: 0.4 ; beta: auto\n",
      "Training Time: 89.03 seconds, Response Time: 0.21 seconds, Coherence Score: 0.4145923169148483\n",
      "\n",
      "purity_score: 0.3885778275475924\n",
      "NMI: 0.3092693538513533\n",
      "Coherence Score: 0.5760583263582043\n",
      "i: 10 ; alpha: 0.4 ; beta: 0.4\n",
      "Training Time: 80.38 seconds, Response Time: 0.21 seconds, Coherence Score: 0.5760583263582043\n",
      "\n",
      "purity_score: 0.24860022396416573\n",
      "NMI: 0.008871973299616111\n",
      "Coherence Score: 0.5536451258514451\n",
      "i: 10 ; alpha: 0.4 ; beta: 0.7\n",
      "Training Time: 66.13 seconds, Response Time: 0.19 seconds, Coherence Score: 0.5536451258514451\n",
      "\n",
      "purity_score: 0.5587905935050392\n",
      "NMI: 0.4054106749262317\n",
      "Coherence Score: 0.4263998131463706\n",
      "i: 10 ; alpha: 0.7 ; beta: auto\n",
      "Training Time: 74.04 seconds, Response Time: 0.18 seconds, Coherence Score: 0.4263998131463706\n",
      "\n",
      "purity_score: 0.3986562150055991\n",
      "NMI: 0.3155308480398449\n",
      "Coherence Score: 0.5609231337179496\n",
      "i: 10 ; alpha: 0.7 ; beta: 0.4\n",
      "Training Time: 68.23 seconds, Response Time: 0.20 seconds, Coherence Score: 0.5609231337179496\n",
      "\n",
      "purity_score: 0.24860022396416573\n",
      "NMI: 0.008871973299616111\n",
      "Coherence Score: 0.5621620116886041\n",
      "i: 10 ; alpha: 0.7 ; beta: 0.7\n",
      "Training Time: 66.06 seconds, Response Time: 0.18 seconds, Coherence Score: 0.5621620116886041\n",
      "\n",
      "purity_score: 0.3818589025755879\n",
      "NMI: 0.2571089900208521\n",
      "Coherence Score: 0.41048589113613937\n",
      "i: 30 ; alpha: symmetric ; beta: auto\n",
      "Training Time: 97.34 seconds, Response Time: 0.27 seconds, Coherence Score: 0.41048589113613937\n",
      "\n",
      "purity_score: 0.2799552071668533\n",
      "NMI: 0.08790611288415313\n",
      "Coherence Score: 0.4905054316147877\n",
      "i: 30 ; alpha: symmetric ; beta: 0.4\n",
      "Training Time: 78.75 seconds, Response Time: 0.26 seconds, Coherence Score: 0.4905054316147877\n",
      "\n",
      "purity_score: 0.24748040313549832\n",
      "NMI: 0.006474976925318856\n",
      "Coherence Score: 0.5448010463563093\n",
      "i: 30 ; alpha: symmetric ; beta: 0.7\n",
      "Training Time: 69.87 seconds, Response Time: 0.20 seconds, Coherence Score: 0.5448010463563093\n",
      "\n",
      "purity_score: 0.4244120940649496\n",
      "NMI: 0.29757114839019416\n",
      "Coherence Score: 0.48361083576770264\n",
      "i: 30 ; alpha: 0.4 ; beta: auto\n",
      "Training Time: 100.51 seconds, Response Time: 0.29 seconds, Coherence Score: 0.48361083576770264\n",
      "\n",
      "purity_score: 0.4076147816349384\n",
      "NMI: 0.2641702525896778\n",
      "Coherence Score: 0.557723074085066\n",
      "i: 30 ; alpha: 0.4 ; beta: 0.4\n",
      "Training Time: 81.30 seconds, Response Time: 0.26 seconds, Coherence Score: 0.557723074085066\n",
      "\n",
      "purity_score: 0.24860022396416573\n",
      "NMI: 0.008871973299616111\n",
      "Coherence Score: 0.5215676511008218\n",
      "i: 30 ; alpha: 0.4 ; beta: 0.7\n",
      "Training Time: 70.35 seconds, Response Time: 0.19 seconds, Coherence Score: 0.5215676511008218\n",
      "\n",
      "purity_score: 0.4501679731243001\n",
      "NMI: 0.3161238301664427\n",
      "Coherence Score: 0.48391220531310486\n",
      "i: 30 ; alpha: 0.7 ; beta: auto\n",
      "Training Time: 96.11 seconds, Response Time: 0.26 seconds, Coherence Score: 0.48391220531310486\n",
      "\n",
      "purity_score: 0.4400895856662934\n",
      "NMI: 0.33258741499989203\n",
      "Coherence Score: 0.5480360301822873\n",
      "i: 30 ; alpha: 0.7 ; beta: 0.4\n",
      "Training Time: 85.28 seconds, Response Time: 0.33 seconds, Coherence Score: 0.5480360301822873\n",
      "\n",
      "purity_score: 0.24860022396416573\n",
      "NMI: 0.008871973299616111\n",
      "Coherence Score: 0.6122877647239445\n",
      "i: 30 ; alpha: 0.7 ; beta: 0.7\n",
      "Training Time: 73.61 seconds, Response Time: 0.19 seconds, Coherence Score: 0.6122877647239445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df = df_newsgroups_split\n",
    "\n",
    "# Define the function with added timing\n",
    "def calculate_coherence_score(i, alpha, beta):\n",
    "    # Measure training time\n",
    "    start_train = time.time()\n",
    "    lda_result = LDA(corpus=df[df['div']=='train']['corpus'], id2word=dictionary,\n",
    "                     iterations=i, num_topics=TOPICS, \n",
    "                     chunksize=2000, random_state=42, gamma_threshold=0.001,\n",
    "                     passes=10, update_every=1,\n",
    "                     alpha=alpha, eta=beta)\n",
    "    end_train = time.time()\n",
    "    training_time = end_train - start_train\n",
    "\n",
    "    # Measure response time\n",
    "    start_response = time.time()\n",
    "    test_corpus_bow = df[df['div']=='test']['corpus'].to_numpy()\n",
    "    test_res = lda_result[test_corpus_bow]\n",
    "\n",
    "    # Get predictions for test set\n",
    "    pred = []\n",
    "    for x in test_res:\n",
    "        x = {k[0]: k[1] for k in x}\n",
    "        pred.append(max(x, key=x.get))\n",
    "    end_response = time.time()\n",
    "    response_time = end_response - start_response\n",
    "\n",
    "    # Load true labels and calculate metrics\n",
    "    y_true = df[df['div']=='test']['label']\n",
    "    y_pred = pred\n",
    "    q_metrics(y_true, y_pred)\n",
    "\n",
    "    # Calculate and print coherence score\n",
    "    cm_lda = CoherenceModel(model=lda_result, dictionary=dictionary, \n",
    "                            corpus=df[(df['div']=='train')]['corpus'], \n",
    "                            texts=df[df['div']=='train']['tok'].to_numpy(), \n",
    "                            coherence='c_v')\n",
    "    coherence_lda = cm_lda.get_coherence()\n",
    "    print(f\"Coherence Score: {coherence_lda}\")\n",
    "\n",
    "    return training_time, response_time, coherence_lda\n",
    "\n",
    "# List of various hyperparameters\n",
    "no_of_iteration = [10, 30]\n",
    "alpha_list = ['symmetric', 0.4, 0.7]\n",
    "beta_list = ['auto', 0.4, 0.7]\n",
    "\n",
    "# Running the parameter grid search with timing\n",
    "for i in no_of_iteration:\n",
    "    for alpha in alpha_list:\n",
    "        for beta in beta_list:\n",
    "            training_time, response_time, coherence_lda = calculate_coherence_score(i, alpha, beta)\n",
    "            print(f\"i: {i} ; alpha: {alpha} ; beta: {beta}\")\n",
    "            print(f\"Training Time: {training_time:.2f} seconds, Response Time: {response_time:.2f} seconds, Coherence Score: {coherence_lda}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing Parameter Selection for Best Model Performance Topics 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_score: 0.49832026875699886\n",
      "NMI: 0.38364380043795926\n",
      "Coherence Score: 0.3569278850799392\n",
      "i: 10 ; alpha: symmetric ; beta: auto\n",
      "Training Time: 82.29 seconds, Response Time: 0.26 seconds, Coherence Score: 0.3569278850799392\n",
      "\n",
      "purity_score: 0.4244120940649496\n",
      "NMI: 0.33677407434736106\n",
      "Coherence Score: 0.4738549416604432\n",
      "i: 10 ; alpha: symmetric ; beta: 0.4\n",
      "Training Time: 73.53 seconds, Response Time: 0.25 seconds, Coherence Score: 0.4738549416604432\n",
      "\n",
      "purity_score: 0.3964165733482643\n",
      "NMI: 0.24623218656545398\n",
      "Coherence Score: 0.4620343987803395\n",
      "i: 10 ; alpha: symmetric ; beta: 0.7\n",
      "Training Time: 70.15 seconds, Response Time: 0.27 seconds, Coherence Score: 0.4620343987803395\n",
      "\n",
      "purity_score: 0.49496080627099664\n",
      "NMI: 0.3680480902851045\n",
      "Coherence Score: 0.4244610080067536\n",
      "i: 10 ; alpha: 0.4 ; beta: auto\n",
      "Training Time: 84.59 seconds, Response Time: 0.26 seconds, Coherence Score: 0.4244610080067536\n",
      "\n",
      "purity_score: 0.42105263157894735\n",
      "NMI: 0.3301952618330405\n",
      "Coherence Score: 0.4676875530419857\n",
      "i: 10 ; alpha: 0.4 ; beta: 0.4\n",
      "Training Time: 79.52 seconds, Response Time: 0.25 seconds, Coherence Score: 0.4676875530419857\n",
      "\n",
      "purity_score: 0.38521836506159013\n",
      "NMI: 0.22488683638500986\n",
      "Coherence Score: 0.5068273464474867\n",
      "i: 10 ; alpha: 0.4 ; beta: 0.7\n",
      "Training Time: 77.46 seconds, Response Time: 0.25 seconds, Coherence Score: 0.5068273464474867\n",
      "\n",
      "purity_score: 0.5587905935050392\n",
      "NMI: 0.3837598974432982\n",
      "Coherence Score: 0.4591859492795353\n",
      "i: 10 ; alpha: 0.7 ; beta: auto\n",
      "Training Time: 80.30 seconds, Response Time: 0.24 seconds, Coherence Score: 0.4591859492795353\n",
      "\n",
      "purity_score: 0.4143337066069429\n",
      "NMI: 0.3213890952382653\n",
      "Coherence Score: 0.4883335324016153\n",
      "i: 10 ; alpha: 0.7 ; beta: 0.4\n",
      "Training Time: 71.92 seconds, Response Time: 0.26 seconds, Coherence Score: 0.4883335324016153\n",
      "\n",
      "purity_score: 0.387458006718925\n",
      "NMI: 0.22847716779153543\n",
      "Coherence Score: 0.4607744766414089\n",
      "i: 10 ; alpha: 0.7 ; beta: 0.7\n",
      "Training Time: 70.54 seconds, Response Time: 0.27 seconds, Coherence Score: 0.4607744766414089\n",
      "\n",
      "purity_score: 0.458006718924972\n",
      "NMI: 0.3389149018586479\n",
      "Coherence Score: 0.5019358482071204\n",
      "i: 30 ; alpha: symmetric ; beta: auto\n",
      "Training Time: 111.00 seconds, Response Time: 0.34 seconds, Coherence Score: 0.5019358482071204\n",
      "\n",
      "purity_score: 0.4322508398656215\n",
      "NMI: 0.34068482561031377\n",
      "Coherence Score: 0.3906622566694518\n",
      "i: 30 ; alpha: symmetric ; beta: 0.4\n",
      "Training Time: 97.95 seconds, Response Time: 0.34 seconds, Coherence Score: 0.3906622566694518\n",
      "\n",
      "purity_score: 0.24748040313549832\n",
      "NMI: 0.006474976925318856\n",
      "Coherence Score: 0.552668925742951\n",
      "i: 30 ; alpha: symmetric ; beta: 0.7\n",
      "Training Time: 83.15 seconds, Response Time: 0.23 seconds, Coherence Score: 0.552668925742951\n",
      "\n",
      "purity_score: 0.45464725643896975\n",
      "NMI: 0.3334433988020008\n",
      "Coherence Score: 0.530943623435032\n",
      "i: 30 ; alpha: 0.4 ; beta: auto\n",
      "Training Time: 119.12 seconds, Response Time: 0.35 seconds, Coherence Score: 0.530943623435032\n",
      "\n",
      "purity_score: 0.43561030235162373\n",
      "NMI: 0.3385490487317915\n",
      "Coherence Score: 0.5259851169160916\n",
      "i: 30 ; alpha: 0.4 ; beta: 0.4\n",
      "Training Time: 100.39 seconds, Response Time: 0.36 seconds, Coherence Score: 0.5259851169160916\n",
      "\n",
      "purity_score: 0.24860022396416573\n",
      "NMI: 0.008871973299616111\n",
      "Coherence Score: 0.534782457692593\n",
      "i: 30 ; alpha: 0.4 ; beta: 0.7\n",
      "Training Time: 85.81 seconds, Response Time: 0.26 seconds, Coherence Score: 0.534782457692593\n",
      "\n",
      "purity_score: 0.5240761478163494\n",
      "NMI: 0.36735027897480976\n",
      "Coherence Score: 0.5410797833657343\n",
      "i: 30 ; alpha: 0.7 ; beta: auto\n",
      "Training Time: 114.85 seconds, Response Time: 0.34 seconds, Coherence Score: 0.5410797833657343\n",
      "\n",
      "purity_score: 0.43784994400895855\n",
      "NMI: 0.33328288083302043\n",
      "Coherence Score: 0.5567470147848328\n",
      "i: 30 ; alpha: 0.7 ; beta: 0.4\n",
      "Training Time: 100.18 seconds, Response Time: 0.37 seconds, Coherence Score: 0.5567470147848328\n",
      "\n",
      "purity_score: 0.24860022396416573\n",
      "NMI: 0.008871973299616111\n",
      "Coherence Score: 0.4833571930208319\n",
      "i: 30 ; alpha: 0.7 ; beta: 0.7\n",
      "Training Time: 85.39 seconds, Response Time: 0.27 seconds, Coherence Score: 0.4833571930208319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "df = df_newsgroups_split\n",
    "\n",
    "# Define the function with added timing\n",
    "def calculate_coherence_score(i, alpha, beta):\n",
    "    # Measure training time\n",
    "    start_train = time.time()\n",
    "    lda_result = LDA(corpus=df[df['div']=='train']['corpus'], id2word=dictionary,\n",
    "                     iterations=i, num_topics=TOPICS, \n",
    "                     chunksize=2000, random_state=42, gamma_threshold=0.001,\n",
    "                     passes=10, update_every=1,\n",
    "                     alpha=alpha, eta=beta)\n",
    "    end_train = time.time()\n",
    "    training_time = end_train - start_train\n",
    "\n",
    "    # Measure response time\n",
    "    start_response = time.time()\n",
    "    test_corpus_bow = df[df['div']=='test']['corpus'].to_numpy()\n",
    "    test_res = lda_result[test_corpus_bow]\n",
    "\n",
    "    # Get predictions for test set\n",
    "    pred = []\n",
    "    for x in test_res:\n",
    "        x = {k[0]: k[1] for k in x}\n",
    "        pred.append(max(x, key=x.get))\n",
    "    end_response = time.time()\n",
    "    response_time = end_response - start_response\n",
    "\n",
    "    # Load true labels and calculate metrics\n",
    "    y_true = df[df['div']=='test']['label']\n",
    "    y_pred = pred\n",
    "    q_metrics(y_true, y_pred)\n",
    "\n",
    "    # Calculate and print coherence score\n",
    "    cm_lda = CoherenceModel(model=lda_result, dictionary=dictionary, \n",
    "                            corpus=df[(df['div']=='train')]['corpus'], \n",
    "                            texts=df[df['div']=='train']['tok'].to_numpy(), \n",
    "                            coherence='c_v')\n",
    "    coherence_lda = cm_lda.get_coherence()\n",
    "    print(f\"Coherence Score: {coherence_lda}\")\n",
    "\n",
    "    return training_time, response_time, coherence_lda\n",
    "\n",
    "# List of various hyperparameters\n",
    "no_of_iteration = [10, 30]\n",
    "alpha_list = ['symmetric', 0.4, 0.7]\n",
    "beta_list = ['auto', 0.4, 0.7]\n",
    "\n",
    "# Running the parameter grid search with timing\n",
    "for i in no_of_iteration:\n",
    "    for alpha in alpha_list:\n",
    "        for beta in beta_list:\n",
    "            training_time, response_time, coherence_lda = calculate_coherence_score(i, alpha, beta)\n",
    "            print(f\"i: {i} ; alpha: {alpha} ; beta: {beta}\")\n",
    "            print(f\"Training Time: {training_time:.2f} seconds, Response Time: {response_time:.2f} seconds, Coherence Score: {coherence_lda}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model with Topic 10 \n",
    "i= 10 ; alpha= 0.7 ; beta= auto \\\n",
    "purity_score: 0.558 \\\n",
    "NMI: 0.383 \\\n",
    "Coherence Score: 0.459 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_score: 0.5587905935050392\n",
      "NMI: 0.3837598974432982\n",
      "coherence_lda: 0.4591859492795353\n"
     ]
    }
   ],
   "source": [
    "i= 10 ; alpha= 0.7 ; beta= 'auto'\n",
    "TOPICS= 10\n",
    "# i= 10 ; alpha= 'symmetric' ; beta= 'auto'\n",
    "lda_result = LDA(corpus=df[df['div']=='train']['corpus'], id2word=dictionary,\n",
    "                     iterations=i, num_topics=TOPICS, \n",
    "                     chunksize=2000, random_state=42, gamma_threshold=0.001,\n",
    "                     passes=10, update_every=1,\n",
    "                     alpha=alpha, eta=beta)\n",
    "\n",
    "test_corpus_bow = df[df['div']=='test']['corpus'].to_numpy()\n",
    "test_res = lda_result[test_corpus_bow]\n",
    "\n",
    "pred_test=[]\n",
    "for x in test_res:\n",
    "    x={k[0]:k[1] for k in x}\n",
    "    pred_test.append(max(x,key=x.get) )\n",
    "\n",
    "y_true = df[df['div']=='test']['label'] \n",
    "y_pred = pred_test\n",
    "q_metrics(y_true, y_pred)\n",
    "\n",
    "\n",
    "# evaluate model using Topic Coherence score\n",
    "cm_lda = CoherenceModel(model=lda_result,\n",
    "                          dictionary=dictionary, \n",
    "                          corpus=df[(df['div']=='train')]['corpus'], \n",
    "                          texts=df[df['div']=='train']['tok'].to_numpy(), \n",
    "                          coherence='c_v')\n",
    "\n",
    "coherence_lda = cm_lda.get_coherence()\n",
    "    \n",
    "print('coherence_lda:', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_bow = df[df['div']=='train']['corpus'].to_numpy()\n",
    "train_res = lda_result[train_corpus_bow]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = []\n",
    "for x in train_res:\n",
    "    x = {k[0]: k[1] for k in x}\n",
    "    pred_train.append(max(x, key=x.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "  \n",
    "# Create a DataFrame for training data\n",
    "train_df = pd.DataFrame({\n",
    "    'topic': pred_train,\n",
    "    'label': df_newsgroups_split[df_newsgroups_split['div']=='train']['label']\n",
    "})\n",
    "\n",
    "# Group by 'topic' and count how many of each label there are for each topic in the training data\n",
    "train_topic_label_counts = train_df.groupby(['topic', 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Assign the mode (most frequent label) for each topic in the training data\n",
    "topic_to_mode_label = train_df.groupby('topic')['label'].agg(lambda x: x.mode().iloc[0])\n",
    "\n",
    "# Map the predicted test topics to actual labels using the mapping from training data\n",
    "mapped_test_labels = [topic_to_mode_label.get(topic, None) for topic in pred_test]\n",
    "\n",
    "# Create a DataFrame to combine mapped test labels and actual test labels\n",
    "test_results_df = pd.DataFrame({\n",
    "    'predicted_label': mapped_test_labels,\n",
    "    'true_label':df_newsgroups_split[df_newsgroups_split['div']=='test']['label']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Miscellaneous       0.00      0.00      0.00        46\n",
      "     Politics       0.43      0.69      0.53       127\n",
      "     Religion       0.00      0.00      0.00        66\n",
      "      Science       0.44      0.46      0.45       208\n",
      "        Sport       0.90      0.60      0.72       120\n",
      "   Technology       0.62      0.96      0.76       219\n",
      "     Vehicles       0.60      0.31      0.41       107\n",
      "\n",
      "     accuracy                           0.56       893\n",
      "    macro avg       0.43      0.43      0.41       893\n",
      " weighted avg       0.51      0.56      0.51       893\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patsias/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/patsias/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/patsias/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_results_df['true_label'],test_results_df['predicted_label']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
