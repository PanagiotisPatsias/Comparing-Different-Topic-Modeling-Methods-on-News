{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim.models import CoherenceModel\n",
    "import gensim\n",
    "from gensim.models.ldamodel import LdaModel as LDA\n",
    "from sklearn.metrics import normalized_mutual_info_score, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # Confusion matrix\n",
    "    contingency_matrix = confusion_matrix(y_true, y_pred)\n",
    "    # Find optimal one-to-one mapping between labels and clusters\n",
    "    row_ind, col_ind = linear_sum_assignment(-contingency_matrix)\n",
    "    return contingency_matrix[row_ind, col_ind].sum() / np.sum(contingency_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['category', 'filename', 'title', 'content'], dtype='object')\n",
      "0     Quarterly profits at US media giant TimeWarne...\n",
      "1     The dollar has hit its highest level against ...\n",
      "2     The owners of embattled Russian oil giant Yuk...\n",
      "3     British Airways has blamed high fuel prices f...\n",
      "4     Shares in UK drinks and food firm Allied Dome...\n",
      "Name: content, dtype: object\n",
      "                                             content  category\n",
      "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...         7\n",
      "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...         4\n",
      "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...         4\n",
      "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...         1\n",
      "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...        14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load Datasets\n",
    "# For BBC News dataset\n",
    "bbc_news = pd.read_csv(\"/home/patsias/Essential Text/Comparing-Different-Topic-Modeling-Methods-on-News/bbc-news-data.csv\",sep=\"\\t\")  # Load the BBC dataset\n",
    "print(bbc_news.columns)\n",
    "bbc_texts = bbc_news['content']\n",
    "print(bbc_texts.head())\n",
    "bbc_labels = LabelEncoder().fit_transform(bbc_news['category']) \n",
    "\n",
    "# For 20 Newsgroups dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_texts = newsgroups_train.data\n",
    "newsgroups_labels = newsgroups_train.target\n",
    "\n",
    "df_news = pd.DataFrame({'content': newsgroups_texts, 'category': newsgroups_labels})\n",
    "\n",
    "# Show the first few rows of the newsgroups dataset\n",
    "print(df_news.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 511, 0: 510, 2: 417, 4: 401, 1: 386})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(bbc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 600,\n",
       "         15: 599,\n",
       "         8: 598,\n",
       "         9: 597,\n",
       "         11: 595,\n",
       "         7: 594,\n",
       "         13: 594,\n",
       "         14: 593,\n",
       "         5: 593,\n",
       "         2: 591,\n",
       "         12: 591,\n",
       "         3: 590,\n",
       "         6: 585,\n",
       "         1: 584,\n",
       "         4: 578,\n",
       "         17: 564,\n",
       "         16: 546,\n",
       "         0: 480,\n",
       "         18: 465,\n",
       "         19: 377})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(newsgroups_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess (not need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(texts):\n",
    "    vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')  # Using BoW instead of TF-IDF\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return X, vectorizer#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess BBC dataset\n",
    "X_bbc, vectorizer_bbc = preprocess(bbc_texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Number of topics to extract \n",
    "n_topics = 10\n",
    "no_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>001.txt</td>\n",
       "      <td>Ad sales boost Time Warner profit</td>\n",
       "      <td>Quarterly profits at US media giant TimeWarne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>002.txt</td>\n",
       "      <td>Dollar gains on Greenspan speech</td>\n",
       "      <td>The dollar has hit its highest level against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>003.txt</td>\n",
       "      <td>Yukos unit buyer faces loan claim</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>004.txt</td>\n",
       "      <td>High fuel prices hit BA's profits</td>\n",
       "      <td>British Airways has blamed high fuel prices f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>005.txt</td>\n",
       "      <td>Pernod takeover talk lifts Domecq</td>\n",
       "      <td>Shares in UK drinks and food firm Allied Dome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>397.txt</td>\n",
       "      <td>BT program to beat dialler scams</td>\n",
       "      <td>BT is introducing two initiatives to help bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>398.txt</td>\n",
       "      <td>Spam e-mails tempt net shoppers</td>\n",
       "      <td>Computer users across the world continue to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>399.txt</td>\n",
       "      <td>Be careful how you code</td>\n",
       "      <td>A new European directive could put software w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>400.txt</td>\n",
       "      <td>US cyber security chief resigns</td>\n",
       "      <td>The man making sure US computer networks are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>401.txt</td>\n",
       "      <td>Losing yourself in online gaming</td>\n",
       "      <td>Online role playing games are time-consuming,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category filename                              title  \\\n",
       "0     business  001.txt  Ad sales boost Time Warner profit   \n",
       "1     business  002.txt   Dollar gains on Greenspan speech   \n",
       "2     business  003.txt  Yukos unit buyer faces loan claim   \n",
       "3     business  004.txt  High fuel prices hit BA's profits   \n",
       "4     business  005.txt  Pernod takeover talk lifts Domecq   \n",
       "...        ...      ...                                ...   \n",
       "2220      tech  397.txt   BT program to beat dialler scams   \n",
       "2221      tech  398.txt    Spam e-mails tempt net shoppers   \n",
       "2222      tech  399.txt            Be careful how you code   \n",
       "2223      tech  400.txt    US cyber security chief resigns   \n",
       "2224      tech  401.txt   Losing yourself in online gaming   \n",
       "\n",
       "                                                content  \n",
       "0      Quarterly profits at US media giant TimeWarne...  \n",
       "1      The dollar has hit its highest level against ...  \n",
       "2      The owners of embattled Russian oil giant Yuk...  \n",
       "3      British Airways has blamed high fuel prices f...  \n",
       "4      Shares in UK drinks and food firm Allied Dome...  \n",
       "...                                                 ...  \n",
       "2220   BT is introducing two initiatives to help bea...  \n",
       "2221   Computer users across the world continue to i...  \n",
       "2222   A new European directive could put software w...  \n",
       "2223   The man making sure US computer networks are ...  \n",
       "2224   Online role playing games are time-consuming,...  \n",
       "\n",
       "[2225 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patsias/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation using regex\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text) \n",
    "    \n",
    "    # Tokenize words, remove stopwords, and convert back to string\n",
    "    words = text.split() \n",
    "    words = [word for word in words if word.lower() not in stop_words]  \n",
    "    \n",
    "    # Return preprocessed text as a single string\n",
    "    return \" \".join(words)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "bbc_news['text']=bbc_news.apply(lambda r: r.title + r.content, axis=1) \n",
    "bbc_news['preprocessed_text'] = bbc_news['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news['tok'] = bbc_news['preprocessed_text'].apply(lambda x: set(x.split()))  \n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train and test sets (80% train, 20% test as an example)\n",
    "train, test = train_test_split(bbc_news, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a new column indicating 'train' or 'test'\n",
    "train['div'] = 'train'\n",
    "test['div'] = 'test'\n",
    "\n",
    "# Concatenate the two sets back into a single DataFrame\n",
    "bbc_news_split = pd.concat([train, test])\n",
    "\n",
    "# Optional: Reset index if needed\n",
    "bbc_news_split = bbc_news_split.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = bbc_news_split[bbc_news_split['div']=='train'].tok.to_numpy() \n",
    "dictionary = gensim.corpora.Dictionary(train_docs)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume  the dictionary looks like this: {'machine': 0, 'learning': 1, 'data': 2, 'science': 3, 'deep': 4}   The doc2bow() function will convert each document into a list of tuples: [(0, 1), (1, 1), (2, 1)]  # 'machine' appears 1 time, 'learning' appears 1 time, 'data' appears 1 time\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in bbc_news_split.tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bbc_news_split['corpus']=bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def q_metrics(y_true, y_pred,my_model=None):\n",
    "    contigency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    purity = np.sum(np.amax(contigency_matrix, axis=0)) / np.sum(contigency_matrix)\n",
    "    print('purity_score:',purity)\n",
    "    print('NMI:',metrics.normalized_mutual_info_score(y_true, y_pred))\n",
    "    \n",
    "    if my_model!=None:\n",
    "        cm = CoherenceModel(model=my_model, corpus=bow_corpus, dictionary=dictionary, coherence='u_mass')\n",
    "        print('Coherence:',cm.get_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_news_split['label']=bbc_news_split.category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_score: 0.8112359550561797\n",
      "NMI: 0.6458833678162469\n",
      "coherence_lda: 0.2847665471143438\n",
      "i : 10 ; alpha : symmetric ; beta : auto \n",
      "purity_score: 0.8584269662921349\n",
      "NMI: 0.7161478212910702\n",
      "coherence_lda: 0.29878421813641165\n",
      "i : 10 ; alpha : symmetric ; beta : 0.4 \n",
      "purity_score: 0.8337078651685393\n",
      "NMI: 0.6920769684914702\n",
      "coherence_lda: 0.3137286281602882\n",
      "i : 10 ; alpha : symmetric ; beta : 0.7 \n",
      "purity_score: 0.8134831460674158\n",
      "NMI: 0.6484089842431501\n",
      "coherence_lda: 0.28458481130182767\n",
      "i : 10 ; alpha : 0.4 ; beta : auto \n",
      "purity_score: 0.8584269662921349\n",
      "NMI: 0.7157919961258384\n",
      "coherence_lda: 0.29878421813641165\n",
      "i : 10 ; alpha : 0.4 ; beta : 0.4 \n",
      "purity_score: 0.8359550561797753\n",
      "NMI: 0.6958581305399223\n",
      "coherence_lda: 0.3137286281602882\n",
      "i : 10 ; alpha : 0.4 ; beta : 0.7 \n",
      "purity_score: 0.802247191011236\n",
      "NMI: 0.6378413562694111\n",
      "coherence_lda: 0.27935916887479706\n",
      "i : 10 ; alpha : 0.7 ; beta : auto \n",
      "purity_score: 0.8539325842696629\n",
      "NMI: 0.7115318640961922\n",
      "coherence_lda: 0.29232716137484355\n",
      "i : 10 ; alpha : 0.7 ; beta : 0.4 \n",
      "purity_score: 0.8337078651685393\n",
      "NMI: 0.694616247372786\n",
      "coherence_lda: 0.31328392436293806\n",
      "i : 10 ; alpha : 0.7 ; beta : 0.7 \n",
      "purity_score: 0.7797752808988764\n",
      "NMI: 0.6179905387240756\n",
      "coherence_lda: 0.28517311495415465\n",
      "i : 30 ; alpha : symmetric ; beta : auto \n",
      "purity_score: 0.7640449438202247\n",
      "NMI: 0.6093303100953732\n",
      "coherence_lda: 0.2928685647814779\n",
      "i : 30 ; alpha : symmetric ; beta : 0.4 \n",
      "purity_score: 0.6247191011235955\n",
      "NMI: 0.5572612344976887\n",
      "coherence_lda: 0.3251923718649407\n",
      "i : 30 ; alpha : symmetric ; beta : 0.7 \n",
      "purity_score: 0.7730337078651686\n",
      "NMI: 0.6115548776068265\n",
      "coherence_lda: 0.2929529246211623\n",
      "i : 30 ; alpha : 0.4 ; beta : auto \n",
      "purity_score: 0.7662921348314606\n",
      "NMI: 0.6155634376788209\n",
      "coherence_lda: 0.3016319616516602\n",
      "i : 30 ; alpha : 0.4 ; beta : 0.4 \n",
      "purity_score: 0.6247191011235955\n",
      "NMI: 0.5565407170881193\n",
      "coherence_lda: 0.33822080142712824\n",
      "i : 30 ; alpha : 0.4 ; beta : 0.7 \n",
      "purity_score: 0.7730337078651686\n",
      "NMI: 0.6114823756443947\n",
      "coherence_lda: 0.2871347817199573\n",
      "i : 30 ; alpha : 0.7 ; beta : auto \n",
      "purity_score: 0.7797752808988764\n",
      "NMI: 0.6264161717649712\n",
      "coherence_lda: 0.30419431729766294\n",
      "i : 30 ; alpha : 0.7 ; beta : 0.4 \n",
      "purity_score: 0.6292134831460674\n",
      "NMI: 0.5590857994316278\n",
      "coherence_lda: 0.32444692505075323\n",
      "i : 30 ; alpha : 0.7 ; beta : 0.7 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "df = bbc_news_split\n",
    "TOPICS=5\n",
    "def calculate_coherence_score(i, alpha, beta):\n",
    "    lda_result=LDA(corpus=df[df['div']=='train']['corpus'], id2word=dictionary,\n",
    "               iterations=i , num_topics=TOPICS, \n",
    "               chunksize=2000, random_state=42, gamma_threshold=0.001,\n",
    "               passes=10, update_every=1,\n",
    "               alpha=alpha,eta = beta)\n",
    "\n",
    "    test_corpus_bow = df[df['div']=='test']['corpus'].to_numpy()\n",
    "    test_res = lda_result[test_corpus_bow]\n",
    "\n",
    "    pred=[]\n",
    "    for x in test_res:\n",
    "        x={k[0]:k[1] for k in x}\n",
    "        pred.append(max(x,key=x.get) )\n",
    "\n",
    "    y_true = df[df['div']=='test']['label'] \n",
    "    y_pred = pred\n",
    "    q_metrics(y_true, y_pred)    \n",
    "\n",
    "\n",
    "    cm_lda = CoherenceModel(model=lda_result,\n",
    "                          dictionary=dictionary, \n",
    "                          corpus=df[(df['div']=='train')]['corpus'], \n",
    "                          texts=df[df['div']=='train']['tok'].to_numpy(), \n",
    "                          coherence='c_v')\n",
    "\n",
    "    # get coherence value\n",
    "    coherence_lda = cm_lda.get_coherence()\n",
    "        \n",
    "    print('coherence_lda:', coherence_lda)\n",
    "    return coherence_lda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#list containing various hyperparameters\n",
    "no_of_iteration = [10,30]\n",
    "alpha_list = ['symmetric',0.4,0.7]\n",
    "beta_list = ['auto',0.4,0.7]\n",
    "\n",
    "\n",
    "for i in no_of_iteration:\n",
    "    for alpha in alpha_list:\n",
    "        for beta in beta_list:\n",
    "            calculate_coherence_score(i, alpha, beta)   \n",
    "            print(f\"i : {i} ; alpha : {alpha} ; beta : {beta} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_score: 0.7123595505617978\n",
      "NMI: 0.5248798401515109\n",
      "coherence_lda: 0.23049792783523815\n"
     ]
    }
   ],
   "source": [
    "# i : 30 ; alpha : 0.4 ; beta : 0.7 \n",
    "i : 10 ; alpha : 0.4 ; beta : 0.7\n",
    "lda_result=LDA(corpus=df[(df['div']=='train')]['corpus'], id2word=dictionary,\n",
    "               iterations=i , num_topics=TOPICS,\n",
    "               chunksize=2000, random_state=42, gamma_threshold=0.001,\n",
    "               passes=10, update_every=1,\n",
    "               alpha=alpha,eta = beta)\n",
    "\n",
    "test_corpus_bow = df[df['div']=='test']['corpus'].to_numpy()\n",
    "test_res = lda_result[test_corpus_bow]\n",
    "\n",
    "pred=[]\n",
    "for x in test_res:\n",
    "    x={k[0]:k[1] for k in x}\n",
    "    pred.append(max(x,key=x.get) )\n",
    "\n",
    "y_true = df[df['div']=='test']['label'] \n",
    "y_pred = pred\n",
    "q_metrics(y_true, y_pred)\n",
    "\n",
    "\n",
    "# evaluate model using Topic Coherence score\n",
    "cm_lda = CoherenceModel(model=lda_result,\n",
    "                          dictionary=dictionary, \n",
    "                          corpus=df[(df['div']=='train')]['corpus'], \n",
    "                          texts=df[df['div']=='train']['tok'].to_numpy(), \n",
    "                          coherence='c_v')\n",
    "\n",
    "# get coherence value\n",
    "coherence_lda = cm_lda.get_coherence()\n",
    "print('coherence_lda:', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t Counter({'entertainment': 59, 'sport': 38, 'tech': 2})\n",
      "1 \t Counter({'sport': 56, 'tech': 2, 'business': 1})\n",
      "2 \t Counter({'business': 86, 'tech': 28, 'politics': 3, 'entertainment': 1})\n",
      "3 \t Counter({'tech': 40, 'entertainment': 6, 'business': 5})\n",
      "4 \t Counter({'politics': 73, 'business': 23, 'sport': 8, 'tech': 8, 'entertainment': 6})\n"
     ]
    }
   ],
   "source": [
    "pred_test=[]\n",
    "for x in test_res:\n",
    "    x={k[0]:k[1] for k in x}\n",
    "    pred_test.append(max(x,key=x.get) )\n",
    "    \n",
    "temp = pd.DataFrame()\n",
    "temp['y_true'] = y_true\n",
    "temp['y_pred'] = pred_test\n",
    "for i in range(TOPICS):\n",
    "    print(i,'\\t',Counter(temp[temp['y_pred']==i]['y_true']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>also</td>\n",
       "      <td>one</td>\n",
       "      <td>last</td>\n",
       "      <td>new</td>\n",
       "      <td>year</td>\n",
       "      <td>three</td>\n",
       "      <td>would</td>\n",
       "      <td>years</td>\n",
       "      <td>two</td>\n",
       "      <td>first</td>\n",
       "      <td>make</td>\n",
       "      <td>us</td>\n",
       "      <td>people</td>\n",
       "      <td>world</td>\n",
       "      <td>win</td>\n",
       "      <td>get</td>\n",
       "      <td>could</td>\n",
       "      <td>time</td>\n",
       "      <td>made</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>said</td>\n",
       "      <td>one</td>\n",
       "      <td>also</td>\n",
       "      <td>last</td>\n",
       "      <td>first</td>\n",
       "      <td>us</td>\n",
       "      <td>years</td>\n",
       "      <td>mr</td>\n",
       "      <td>would</td>\n",
       "      <td>time</td>\n",
       "      <td>made</td>\n",
       "      <td>two</td>\n",
       "      <td>year</td>\n",
       "      <td>could</td>\n",
       "      <td>set</td>\n",
       "      <td>going</td>\n",
       "      <td>added</td>\n",
       "      <td>world</td>\n",
       "      <td>three</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said</td>\n",
       "      <td>would</td>\n",
       "      <td>also</td>\n",
       "      <td>new</td>\n",
       "      <td>could</td>\n",
       "      <td>one</td>\n",
       "      <td>year</td>\n",
       "      <td>two</td>\n",
       "      <td>us</td>\n",
       "      <td>first</td>\n",
       "      <td>last</td>\n",
       "      <td>years</td>\n",
       "      <td>people</td>\n",
       "      <td>time</td>\n",
       "      <td>mr</td>\n",
       "      <td>way</td>\n",
       "      <td>next</td>\n",
       "      <td>back</td>\n",
       "      <td>many</td>\n",
       "      <td>told</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said</td>\n",
       "      <td>would</td>\n",
       "      <td>also</td>\n",
       "      <td>new</td>\n",
       "      <td>one</td>\n",
       "      <td>year</td>\n",
       "      <td>years</td>\n",
       "      <td>us</td>\n",
       "      <td>two</td>\n",
       "      <td>people</td>\n",
       "      <td>last</td>\n",
       "      <td>could</td>\n",
       "      <td>make</td>\n",
       "      <td>time</td>\n",
       "      <td>first</td>\n",
       "      <td>mr</td>\n",
       "      <td>world</td>\n",
       "      <td>three</td>\n",
       "      <td>made</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said</td>\n",
       "      <td>would</td>\n",
       "      <td>mr</td>\n",
       "      <td>also</td>\n",
       "      <td>new</td>\n",
       "      <td>could</td>\n",
       "      <td>told</td>\n",
       "      <td>year</td>\n",
       "      <td>people</td>\n",
       "      <td>time</td>\n",
       "      <td>last</td>\n",
       "      <td>us</td>\n",
       "      <td>government</td>\n",
       "      <td>first</td>\n",
       "      <td>one</td>\n",
       "      <td>two</td>\n",
       "      <td>say</td>\n",
       "      <td>made</td>\n",
       "      <td>added</td>\n",
       "      <td>expected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1     2     3      4      5      6      7       8       9      10  \\\n",
       "0  said   also   one  last    new   year  three  would   years     two  first   \n",
       "1  said    one  also  last  first     us  years     mr   would    time   made   \n",
       "2  said  would  also   new  could    one   year    two      us   first   last   \n",
       "3  said  would  also   new    one   year  years     us     two  people   last   \n",
       "4  said  would    mr  also    new  could   told   year  people    time   last   \n",
       "\n",
       "      11          12      13     14     15     16     17     18        19  \n",
       "0   make          us  people  world    win    get  could   time      made  \n",
       "1    two        year   could    set  going  added  world  three       new  \n",
       "2  years      people    time     mr    way   next   back   many      told  \n",
       "3  could        make    time  first     mr  world  three   made      back  \n",
       "4     us  government   first    one    two    say   made  added  expected  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words=[]\n",
    "for i in range(TOPICS):\n",
    "    tt = lda_result.get_topic_terms(i,20)\n",
    "    topic_words.append([dictionary[pair[0]] for pair in tt])\n",
    "df_topwords=pd.DataFrame(topic_words)\n",
    "# df_topwords=df_topwords.T\n",
    "df_topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>tok</th>\n",
       "      <th>div</th>\n",
       "      <th>corpus</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>178.txt</td>\n",
       "      <td>Cole refuses to blame van Persie</td>\n",
       "      <td>Ashley Cole has refused to blame Robin van Pe...</td>\n",
       "      <td>Cole refuses to blame van Persie Ashley Cole h...</td>\n",
       "      <td>cole refuses blame van persie ashley cole refu...</td>\n",
       "      <td>{cup, jeremie, things, added, arsenal, aliadie...</td>\n",
       "      <td>train</td>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech</td>\n",
       "      <td>178.txt</td>\n",
       "      <td>Slimmer PlayStation triple sales</td>\n",
       "      <td>Sony PlayStation 2's slimmer shape has proved...</td>\n",
       "      <td>Slimmer PlayStation triple sales Sony PlayStat...</td>\n",
       "      <td>slimmer playstation triple sales sony playstat...</td>\n",
       "      <td>{obviously, games, comparison, runup, seen, gt...</td>\n",
       "      <td>train</td>\n",
       "      <td>[(1, 1), (5, 1), (26, 1), (49, 1), (59, 1), (7...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>260.txt</td>\n",
       "      <td>Bellamy fined after row</td>\n",
       "      <td>Newcastle have fined their Welsh striker Crai...</td>\n",
       "      <td>Bellamy fined after row Newcastle have fined t...</td>\n",
       "      <td>bellamy fined row newcastle fined welsh strike...</td>\n",
       "      <td>{theres, boiled, media, asked, souness, 25, re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[(5, 1), (9, 1), (24, 1), (26, 1), (42, 1), (5...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>017.txt</td>\n",
       "      <td>Finding new homes for old phones</td>\n",
       "      <td>Re-using old mobile phones is not just good f...</td>\n",
       "      <td>Finding new homes for old phones Re-using old ...</td>\n",
       "      <td>finding new homes old phones reusing old mobil...</td>\n",
       "      <td>{collect, theres, future, romania, divide, 90,...</td>\n",
       "      <td>train</td>\n",
       "      <td>[(1, 1), (26, 1), (35, 1), (36, 1), (52, 1), (...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>101.txt</td>\n",
       "      <td>Sundance to honour foreign films</td>\n",
       "      <td>International films will be given the same pr...</td>\n",
       "      <td>Sundance to honour foreign films International...</td>\n",
       "      <td>sundance honour foreign films international fi...</td>\n",
       "      <td>{theme, redford, directed, siege, benjamin, 20...</td>\n",
       "      <td>train</td>\n",
       "      <td>[(72, 1), (77, 1), (96, 1), (98, 1), (131, 1),...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>232.txt</td>\n",
       "      <td>Connick Jr to lead Broadway show</td>\n",
       "      <td>Singer and actor Harry Connick Jr is to star ...</td>\n",
       "      <td>Connick Jr to lead Broadway show Singer and ac...</td>\n",
       "      <td>connick jr lead broadway show singer actor har...</td>\n",
       "      <td>{raquin, unrest, shows, grace, added, starred,...</td>\n",
       "      <td>test</td>\n",
       "      <td>[(1, 1), (77, 1), (98, 1), (110, 1), (167, 1),...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>business</td>\n",
       "      <td>206.txt</td>\n",
       "      <td>Standard Life cuts policy bonuses</td>\n",
       "      <td>Standard Life, Europe's largest mutual life i...</td>\n",
       "      <td>Standard Life cuts policy bonuses Standard Lif...</td>\n",
       "      <td>standard life cuts policy bonuses standard lif...</td>\n",
       "      <td>{feel, 2006, stand, 25, trim, bonus, added, pr...</td>\n",
       "      <td>test</td>\n",
       "      <td>[(1, 1), (36, 1), (77, 1), (96, 1), (98, 1), (...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>politics</td>\n",
       "      <td>207.txt</td>\n",
       "      <td>February poll claim 'speculation'</td>\n",
       "      <td>Reports that Tony Blair is planning a snap ge...</td>\n",
       "      <td>February poll claim 'speculation' Reports that...</td>\n",
       "      <td>february poll claim speculation reports tony b...</td>\n",
       "      <td>{britain, less, reports, government, lead, maj...</td>\n",
       "      <td>test</td>\n",
       "      <td>[(8, 1), (74, 1), (77, 1), (78, 1), (97, 1), (...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>159.txt</td>\n",
       "      <td>Band Aid 20 single storms to No 1</td>\n",
       "      <td>The new version of the Band Aid song Do They ...</td>\n",
       "      <td>Band Aid 20 single storms to No 1 The new vers...</td>\n",
       "      <td>band aid 20 single storms 1 new version band a...</td>\n",
       "      <td>{hiv, 1985, dionne, number, appear, company, d...</td>\n",
       "      <td>test</td>\n",
       "      <td>[(5, 1), (35, 1), (59, 1), (66, 1), (77, 1), (...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>business</td>\n",
       "      <td>480.txt</td>\n",
       "      <td>Wipro beats forecasts once again</td>\n",
       "      <td>Wipro, India's third-biggest software firm, h...</td>\n",
       "      <td>Wipro beats forecasts once again Wipro, India'...</td>\n",
       "      <td>wipro beats forecasts wipro indias thirdbigges...</td>\n",
       "      <td>{half, problem, 90, control, process, net, pri...</td>\n",
       "      <td>test</td>\n",
       "      <td>[(36, 1), (52, 1), (59, 1), (77, 1), (150, 1),...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category filename                              title  \\\n",
       "0             sport  178.txt   Cole refuses to blame van Persie   \n",
       "1              tech  178.txt   Slimmer PlayStation triple sales   \n",
       "2             sport  260.txt            Bellamy fined after row   \n",
       "3              tech  017.txt   Finding new homes for old phones   \n",
       "4     entertainment  101.txt   Sundance to honour foreign films   \n",
       "...             ...      ...                                ...   \n",
       "2220  entertainment  232.txt   Connick Jr to lead Broadway show   \n",
       "2221       business  206.txt  Standard Life cuts policy bonuses   \n",
       "2222       politics  207.txt  February poll claim 'speculation'   \n",
       "2223  entertainment  159.txt  Band Aid 20 single storms to No 1   \n",
       "2224       business  480.txt   Wipro beats forecasts once again   \n",
       "\n",
       "                                                content  \\\n",
       "0      Ashley Cole has refused to blame Robin van Pe...   \n",
       "1      Sony PlayStation 2's slimmer shape has proved...   \n",
       "2      Newcastle have fined their Welsh striker Crai...   \n",
       "3      Re-using old mobile phones is not just good f...   \n",
       "4      International films will be given the same pr...   \n",
       "...                                                 ...   \n",
       "2220   Singer and actor Harry Connick Jr is to star ...   \n",
       "2221   Standard Life, Europe's largest mutual life i...   \n",
       "2222   Reports that Tony Blair is planning a snap ge...   \n",
       "2223   The new version of the Band Aid song Do They ...   \n",
       "2224   Wipro, India's third-biggest software firm, h...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Cole refuses to blame van Persie Ashley Cole h...   \n",
       "1     Slimmer PlayStation triple sales Sony PlayStat...   \n",
       "2     Bellamy fined after row Newcastle have fined t...   \n",
       "3     Finding new homes for old phones Re-using old ...   \n",
       "4     Sundance to honour foreign films International...   \n",
       "...                                                 ...   \n",
       "2220  Connick Jr to lead Broadway show Singer and ac...   \n",
       "2221  Standard Life cuts policy bonuses Standard Lif...   \n",
       "2222  February poll claim 'speculation' Reports that...   \n",
       "2223  Band Aid 20 single storms to No 1 The new vers...   \n",
       "2224  Wipro beats forecasts once again Wipro, India'...   \n",
       "\n",
       "                                      preprocessed_text  \\\n",
       "0     cole refuses blame van persie ashley cole refu...   \n",
       "1     slimmer playstation triple sales sony playstat...   \n",
       "2     bellamy fined row newcastle fined welsh strike...   \n",
       "3     finding new homes old phones reusing old mobil...   \n",
       "4     sundance honour foreign films international fi...   \n",
       "...                                                 ...   \n",
       "2220  connick jr lead broadway show singer actor har...   \n",
       "2221  standard life cuts policy bonuses standard lif...   \n",
       "2222  february poll claim speculation reports tony b...   \n",
       "2223  band aid 20 single storms 1 new version band a...   \n",
       "2224  wipro beats forecasts wipro indias thirdbigges...   \n",
       "\n",
       "                                                    tok    div  \\\n",
       "0     {cup, jeremie, things, added, arsenal, aliadie...  train   \n",
       "1     {obviously, games, comparison, runup, seen, gt...  train   \n",
       "2     {theres, boiled, media, asked, souness, 25, re...  train   \n",
       "3     {collect, theres, future, romania, divide, 90,...  train   \n",
       "4     {theme, redford, directed, siege, benjamin, 20...  train   \n",
       "...                                                 ...    ...   \n",
       "2220  {raquin, unrest, shows, grace, added, starred,...   test   \n",
       "2221  {feel, 2006, stand, 25, trim, bonus, added, pr...   test   \n",
       "2222  {britain, less, reports, government, lead, maj...   test   \n",
       "2223  {hiv, 1985, dionne, number, appear, company, d...   test   \n",
       "2224  {half, problem, 90, control, process, net, pri...   test   \n",
       "\n",
       "                                                 corpus          label  \n",
       "0     [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...          sport  \n",
       "1     [(1, 1), (5, 1), (26, 1), (49, 1), (59, 1), (7...           tech  \n",
       "2     [(5, 1), (9, 1), (24, 1), (26, 1), (42, 1), (5...          sport  \n",
       "3     [(1, 1), (26, 1), (35, 1), (36, 1), (52, 1), (...           tech  \n",
       "4     [(72, 1), (77, 1), (96, 1), (98, 1), (131, 1),...  entertainment  \n",
       "...                                                 ...            ...  \n",
       "2220  [(1, 1), (77, 1), (98, 1), (110, 1), (167, 1),...  entertainment  \n",
       "2221  [(1, 1), (36, 1), (77, 1), (96, 1), (98, 1), (...       business  \n",
       "2222  [(8, 1), (74, 1), (77, 1), (78, 1), (97, 1), (...       politics  \n",
       "2223  [(5, 1), (35, 1), (59, 1), (66, 1), (77, 1), (...  entertainment  \n",
       "2224  [(36, 1), (52, 1), (59, 1), (77, 1), (150, 1),...       business  \n",
       "\n",
       "[2225 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purity_score: 0.7123595505617978\n",
      "NMI: 0.5248798401515109\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.73      0.72      0.73       115\n",
      "entertainment       0.61      0.83      0.71        72\n",
      "     politics       0.63      0.97      0.77        76\n",
      "        sport       0.98      0.52      0.68       102\n",
      "         tech       0.75      0.59      0.66        80\n",
      "\n",
      "     accuracy                           0.71       445\n",
      "    macro avg       0.74      0.73      0.71       445\n",
      " weighted avg       0.76      0.71      0.71       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_name={0:'entertainment',1:'sport',2:'business',3:'tech',\n",
    "            4:'politics'}\n",
    "\n",
    "y_true = df[df['div']=='test']['label'].to_list()\n",
    "y_pred = [*map(topic_name.get, pred)]\n",
    "\n",
    "q_metrics(y_true, pred)\n",
    "print(classification_report(y_true,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
