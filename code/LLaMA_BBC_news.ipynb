{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import asyncio\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "load_dotenv()\n",
    "LLAMA = os.getenv(\"LLAMA\")\n",
    "login(token=LLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      category                                            content  \\\n",
      "1509     sport  FA probes crowd trouble\\n\\nThe FA is to take a...   \n",
      "1801     sport  Safin cool on Wimbledon\\n\\nNewly-crowned Austr...   \n",
      "136   business  Bank set to leave rates on hold\\n\\nUK interest...   \n",
      "1378     sport  Britain boosted by Holmes double\\n\\nAthletics ...   \n",
      "2183      tech  Confusion over high-definition TV\\n\\nNow that ...   \n",
      "\n",
      "                      name   div  \n",
      "1509     bbc_sport_197.txt  test  \n",
      "1801     bbc_sport_489.txt  test  \n",
      "136   bbc_business_137.txt  test  \n",
      "1378     bbc_sport_066.txt  test  \n",
      "2183      bbc_tech_360.txt  test  \n"
     ]
    }
   ],
   "source": [
    "#Laptop Path\n",
    "#source_path_for_bbc = r\"C:\\Users\\elias\\Documents\\NLP and speech processing\"\n",
    "#Desktop Path\n",
    "source_path_for_bbc = r\"C:\\Users\\super161\\Documents\\NLP and speech processing\"\n",
    "\n",
    "\n",
    "def process_folders(folder_path):\n",
    "    \n",
    "    documents_dict = {}\n",
    "    document_list = []\n",
    "    for folder_name in os.listdir(folder_path):\n",
    "        folder_full_path = os.path.join(folder_path, folder_name)\n",
    "        documents_dict[folder_name] = []\n",
    "        \n",
    "        for document_name in os.listdir(folder_full_path):\n",
    "            document_full_path = os.path.join(folder_full_path, document_name)\n",
    "            \n",
    "            # Open and read the content of each document\n",
    "            with open(document_full_path, 'r', encoding='utf-8') as file:\n",
    "                document_content = file.read()\n",
    "            document_list.append((folder_name, document_content, f\"bbc_{folder_name}_{document_name}\"))\n",
    "        \n",
    "    bbc_news = pd.DataFrame(document_list, columns = ['category', 'content', 'name'])\n",
    "\n",
    "    train, test = train_test_split(bbc_news, test_size=0.2)\n",
    "\n",
    "    train = train.sort_values(by='category')\n",
    "    train_first_doc = train.groupby('category').first().reset_index()\n",
    "\n",
    "    train_first_doc['div'] = 'train'\n",
    "    test['div'] = 'test'\n",
    "\n",
    "\n",
    "    return test, train_first_doc\n",
    "\n",
    "#Data Structure: Topic, Article Text\n",
    "\n",
    "test_df, instruction_df = process_folders(source_path_for_bbc)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n"
     ]
    }
   ],
   "source": [
    "def make_prompts(bbc_instructions, bbc_data):\n",
    "    prompts = []\n",
    "    \n",
    "    # General instructions and fixed texts\n",
    "    general_instruction = (\n",
    "        \"You are a perfect topic modeling machine. Given a text and the different topics, \"\n",
    "        \"you will classify the texts to the correct topic. First you will receive the topics, \"\n",
    "        \"afterwards an example and finally the text you have to assign one of the before mentioned topics to.\"\n",
    "    )\n",
    "    topics = \"The topics are business, entertainment, politics, sport and tech. Please make sure, you know the topics and their meaning.\"\n",
    "    transition_to_examples = \"Now an example for each of the categories will follow.\"\n",
    "    transition_to_text_to_classify = (\n",
    "        \"Now the text, you have to classify will follow. Please assess its topic and answer only the topic of it.\"\n",
    "    )\n",
    "\n",
    "    # Iterate through the test DataFrame rows\n",
    "    for _, test_row in bbc_data.iterrows():\n",
    "        prompt = general_instruction + \"\\n\" + topics + \"\\n\" + transition_to_examples + \"\\n\"\n",
    "\n",
    "        # Iterate through instruction DataFrame to add examples\n",
    "        for _, instruction_row in bbc_instructions.iterrows():\n",
    "            category = instruction_row['category']\n",
    "            example_text = instruction_row['content']\n",
    "            prompt += f\"For the following text: \\n{example_text}\\nThe correct answer would be: {category}\\n\"\n",
    "\n",
    "        # Add the actual text to classify from the test set\n",
    "        text_to_classify = test_row['content']\n",
    "        prompt += transition_to_text_to_classify + \"\\n\" + text_to_classify + \"\\n\"\n",
    "        name = f\"{test_row['name']}\"\n",
    "        #print(name)\n",
    "        prompt_dict = {}\n",
    "        prompt_dict[name] = prompt\n",
    "        prompts.append(prompt_dict)\n",
    "    return prompts            \n",
    "\n",
    "prompts = make_prompts(instruction_df, test_df)\n",
    "\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLACEHOLDER for Prompting LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_correct_anser(results, test_df):\n",
    "    result_with_correct_answer = []\n",
    "    for result in results:\n",
    "       for key, value in result.items():\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            value = value.replace(\" \\n\", \"\")\n",
    "            value = value.replace(\" \",\"\")\n",
    "            value = value.lower()\n",
    "            matching_rows = test_df.loc[test_df['name'] == key, 'category']\n",
    "            \n",
    "            if not matching_rows.empty:\n",
    "                category_value = matching_rows.values[0]\n",
    "                category_value = category_value.lower()\n",
    "                result_with_correct_answer.append({key: (value, category_value)})\n",
    "            else:\n",
    "                print(f\"No matching category found for key: {key}\")\n",
    "                result_with_correct_answer.append({key: (value, None)})\n",
    "    \n",
    "    return result_with_correct_answer\n",
    "\n",
    "results_with_correct_answer = add_correct_anser(results, test_df)\n",
    "print(results_with_correct_answer)\n",
    "print(len(results_with_correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ground_truth_and_predictions(results_with_correct_answer):\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    for result in results_with_correct_answer:\n",
    "        for key, value in result.items():\n",
    "            ground_truth.append(value[1])\n",
    "            predictions.append(value[0])\n",
    "    return ground_truth, predictions\n",
    "ground_truth, predictions = extract_ground_truth_and_predictions(results_with_correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NMI(ground_truth, predictions):\n",
    "    \n",
    "    nmi_score = normalized_mutual_info_score(ground_truth, predictions)\n",
    "    print(f\"Normalized Mutual Information Score: {nmi_score}\")\n",
    "    return nmi_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_purity(predicted_labels, true_labels):\n",
    "    # Convert lists to numpy arrays for easier indexing\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Get unique clusters\n",
    "    unique_clusters = np.unique(predicted_labels)\n",
    "    \n",
    "    # Total number of instances\n",
    "    total_instances = len(true_labels)\n",
    "    \n",
    "    # Calculate the number of correctly classified instances in each cluster\n",
    "    correctly_classified = 0\n",
    "    for cluster in unique_clusters:\n",
    "        # Get the true labels of instances in the current cluster\n",
    "        indices_in_cluster = np.where(predicted_labels == cluster)[0]\n",
    "        labels_in_cluster = true_labels[indices_in_cluster]\n",
    "        \n",
    "        # Determine the most common true label in this cluster\n",
    "        majority_label_count = Counter(labels_in_cluster).most_common(1)[0][1]\n",
    "        \n",
    "        # Add the number of correctly classified instances in this cluster\n",
    "        correctly_classified += majority_label_count\n",
    "    \n",
    "    # Calculate purity\n",
    "    purity = correctly_classified / total_instances\n",
    "    print(f\"Purity: {purity}\")\n",
    "    return purity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted_labels, true_labels):\n",
    "    # Ensure that the predicted_labels and true_labels have the same length\n",
    "    if len(predicted_labels) != len(true_labels):\n",
    "        raise ValueError(\"The length of predicted and true labels must be the same.\")\n",
    "    \n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum(1 for pred, true in zip(predicted_labels, true_labels) if pred == true)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score(ground_truth, predictions):\n",
    "\n",
    "    f1 = f1_score(ground_truth, predictions, average='micro') # Because there might be over/ under representation of some classes\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_NMI(ground_truth, predictions)\n",
    "calculate_purity(predictions, ground_truth)\n",
    "calculate_accuracy(predictions, ground_truth)\n",
    "calculate_f1_score(ground_truth, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
