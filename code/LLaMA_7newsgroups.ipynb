{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                                   12178\n",
      "text             From: uabdpo.dpo.uab.edu!gila005 (Stephen Holl...\n",
      "label20                                                         13\n",
      "labels_name20                                              sci.med\n",
      "label                                                      Science\n",
      "div                                                           test\n",
      "name                                                             6\n",
      "Name: 12178, dtype: object\n",
      "test Length:  3770\n",
      "instruction Length:  7\n",
      "original length:  18846\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import asyncio\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "LLAMA = os.getenv(\"LLAMA\")\n",
    "login(token=LLAMA)\n",
    "\n",
    "\n",
    "# text is regarded as key\n",
    "seven_newsgroups_data = pd.read_csv('fetch_7newsgroups.csv')\n",
    "\n",
    "def split_train_test(data):\n",
    "    train, test = train_test_split(data, test_size=0.2)\n",
    "\n",
    "    train = train.sort_values(by='label')\n",
    "    train_first_doc = train.groupby('label').first().reset_index()\n",
    "\n",
    "    train_first_doc['div'] = 'train'\n",
    "    test['div'] = 'test'\n",
    "\n",
    "    return test, train_first_doc\n",
    "\n",
    "test_df, instruction_df = split_train_test(seven_newsgroups_data)\n",
    "test_df[\"name\"] = range(1, len(test_df) + 1)\n",
    "print(test_df.iloc[5])\n",
    "print(\"test Length: \",len(test_df))\n",
    "print(\"instruction Length: \", len(instruction_df))\n",
    "print(\"original length: \", len(seven_newsgroups_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'You are a perfect topic modeling machine. Given a text and the different topics, you will classify the texts to the correct topic. First you will receive the topics, afterwards an example and finally the text you have to assign one of the before mentioned topics to.\\nThe topics are sport, technology, politics, vehicles, religion, miscellaneous and science. Please make sure, you know the topics and their meaning.\\nNow an example for each of the categories will follow.\\nFor the following text: \\nFrom: tmr@wdl1.wdl.loral.com (Tim Ryan)\\nSubject: ->->->->* Jet Ski Forsale *<-<-<-<-<-\\nOrganization: loral western development labs\\nDistribution: USA\\nLines: 18\\n\\nFor Sale 1989 Kawasaki TS (650) Tandum seating\\nColor: White with Blue and Red.\\nJet Ski runs great and looks good.\\n\\nZiemans Trailer with locking Utility Box.\\nColor: Black\\nZiemans Trailer is less than a year old.\\n\\nBoth have been garaged kept and well maintained.\\n$4200.00 for both \\n(To be sold as a set only)\\n\\nIf interested Call Me at (408) 473-4159 leave message I will return your \\ncall ASAP.\\nThanks,\\nTim Ryan\\nStandard Disclaimer Applies\\n\\n\\nThe correct answer would be: Miscellaneous\\nFor the following text: \\nFrom: rscharfy@magnus.acs.ohio-state.edu (Ryan C Scharfy)\\nSubject: Re: New Study Out On Gay Percentage\\nNntp-Posting-Host: magnusug.magnus.acs.ohio-state.edu\\nOrganization: The Ohio State University\\nLines: 46\\n\\n>>The article also contains numbers on the number of sexual partners.\\n>>The median number of sexual partners for all men 20-39 was 7.3.\\n>>Compared to the table I have already posted from Masters, Johnson,\\n>>and Kolodny showing male homosexual partners, it is apparent that\\n>>homosexual men are dramatically more promiscuous than the general\\n>>male population.  It\\'s a shame that we don\\'t have a breakdown for\\n>>straight men vs. gay/bi men -- that would show even more dramatically\\n>>how much more promiscuous gay/bi men are.\\n>>--\\n>\\n>Isn\\'t is funny how someone who seems to know nothing about homosexuality\\n>uses a very flawed (IMHO) source of information to pass jusgement on all\\n>homosexual and bisexual men.\\n\\nOnly the most comprehensive survey on sexuality in 50 years.\\n\\n>  It would seem more logical to say that since\\n>the heterosexual group of men is larger then the chances of promiscuity\\n>larger as well.  In my opinion, orientation has nothing to do with it.\\n>\\n\\nChance and size have nothing in common on the multimillion number scale we are \\ntalking about.\\n\\n\\n>Men are men and they all like sex.  I am a gay male.  I have had sex three\\n>times in my life, all with the same man.  Before that, I was a virgin.\\n>\\n>So... whose promiscuous?\\n>\\n\\nNobody said that you were.  Chill.\\n\\n>Just because someone is gay doesn\\'t mean they have no morals.  Just because\\n>someone is heterosexual doesn\\'t mean they do.  Look at the world....\\n\\nWell said.\\n\\n>Statistics alone prove that most criminals are by default hetero...\\n>\\n\\nActually, the Kinsley Report in 1947(or 48?) used a high percentage of \\nprisoners so...........\\n\\n\\nRyan\\n\\nThe correct answer would be: Politics\\nFor the following text: \\nFrom: I3150101@dbstu1.rz.tu-bs.de (Benedikt Rosenau)\\nSubject: Re: The Inimitable Rushdie (Re: An Anecdote about Islam\\nOrganization: Technical University Braunschweig, Germany\\nLines: 19\\n\\nIn article <115846@bu.edu>\\njaeger@buphy.bu.edu (Gregg Jaeger) writes:\\n \\n(Deletion)\\n>Certainly. It is a central aspect of Islam to show mercy and to give\\n>those who\\'ve done wrong (even presuming Rushdie _did_ violate Islamic\\n>Law) and committed crimes. This was the basis for my posts regarding\\n>leniency which seemed not to have penetrated Benedikt\\'s skull.\\n \\nYou have demanded harsh punishments of several crimes. Repeating\\noffenders have slipped in only as justification of harsh punishment at\\nall. Typically religious doublespeak. Whenever you have contradictory\\nstatements you choose the possibility that suits your current argument.\\n \\nIt is disgusting that someone with ideas that would make Theodore KKKaldis\\nfeel cozy can go along under the protection of religion.\\n \\nGregg, tell us, would you kill idolaters?\\n   Benedikt\\n\\nThe correct answer would be: Religion\\nFor the following text: \\nFrom: smb@research.att.com (Steven Bellovin)\\nSubject: Re: The Old Key Registration Idea...\\nOrganization: AT&T Bell Laboratories\\nLines: 19\\n\\nIn article <rlglendeC5LrwC.95C@netcom.com>, rlglende@netcom.com (Robert Lewis Glendenning) writes:\\n> I have been chided for stating that Dorthy Denning was intellectually\\n> dishonest in the ACM debate and in this newsgroup.  I have previously\\n> refrained from suggesting that she is arguing on behalf of consulting\\n> clients.\\n> \\n> Now, I say that it is clear that Dorthy Denning has been functioning\\n> as a lobbyist, not a computer scientist.  She has used legal ethics\\n> (truth is what you can convince anyone of), not scientific ethics\\n> (truth is understanding the external world).\\n> \\n> Maybe we can revoke her ACM membership? 8)\\n\\nI suggest that you refrain from such insults unless and until you can\\nproduce some evidence to back up that claim.  Given the measures proposed\\nor passed in the last year or so, such as S.266 and the scanner ban,\\nher proposal need not be any more than her own attempt at a technical\\nsolution.  It\\'s entirely possible, in fact, that it was the notion of\\nsplitting the key, which came up in the debate, that softened this proposal.\\n\\nThe correct answer would be: Science\\nFor the following text: \\nFrom: doctor8@jhuvms.hcf.jhu.edu (Jason Abner Miller)\\nSubject: Re: John Franco\\nOrganization: The Johns Hopkins University - HCF\\nLines: 30\\nDistribution: na\\nNNTP-Posting-Host: jhuvms.hcf.jhu.edu\\nNews-Software: VAX/VMS VNEWS 1.41    \\n\\nIn article <1993Apr23.174759.182922@zeus.calpoly.edu>, jplee@cymbal.calpoly.edu (Jason Lee) writes...\\n>What\\'s with John Franco?  The Mets are hardly using him.  \\n\\n\\tDon\\'t worry.  This is a perfectly normal state of affairs.  Had they\\nactually been using him, you should be worried.\\n\\n>I heard he was completely recovered, but now I\\'m not so sure.\\n\\n\\tHe\\'s recovered totally from his injury.  That\\'s why he\\'s not\\npitching...so he can rest his arm enough that he can get injured again\\npitching on 38 days rest and then have fun dining in the Diamond Club in\\nShea Stadium while AY struggles every day out there...\\n\\n>If there is anybody out there with information about Franco, I would\\n>appreciate it if you could drop me a line.\\n\\n\\tI\\'ve be quite happy to drop John Franco, just the same.\\n\\tBring back Randy Myers!\\n\\tNo, better make that...Bring Back Neil Allen!\\n\\n> \\n>-- \\n>Jason Lee   jplee@oboe.calpoly.edu   jlee@cash.busfac.calpoly.edu    SF Giants\\n>e ^ i*pi + 1 = 0    The most beautiful equation in mathematics.      Magic\\n>For all sad words of tongue and pen, the saddest are these:          Number:\\n>     \"It might have been.\"            John Greenleaf Whittier        148\\n\\nJason A. Miller\\n\"some doctor guy\"\\nTanana:  1-0, 1.50\\n\\nThe correct answer would be: Sport\\nFor the following text: \\nFrom: Kent.Dalton@FtCollinsCO.NCR.COM (Kent.Dalton)\\nSubject: Re: PoV Ray Related Group NEEDED\\n\\t<1t0maaINNo56@darkstar.UCSC.EDU> <C760AJ.Kxv@cs.vu.nl>\\nOrganization: NCR Microelectronics, Ft. Collins, CO\\nLines: 36\\nIn-reply-to: wlieftin@cs.vu.nl\\'s message of 17 May 93 09:42:18 GMT\\n\\n>>>>> On 17 May 93 09:42:18 GMT, wlieftin@cs.vu.nl (Liefting W) said:\\n\\n\\tLiefting> hed@cats.ucsc.edu (Magic Fingers) writes:\\n\\n\\n>In article <1993May13.011926.4728@exucom.com> cyberman@exucom.com (Stephen R.\\n>Phillips) writes:\\n\\n>If it takes making it an alt group, then why not?  I\\'ve been following this\\n>thread for, what has it been, two months now?\\n\\nLiefting> The alt.* hierarchie is created for 2 purposes: 1. For\\nLiefting> groups which do not fit under the comp.* or other \\'official\\'\\nLiefting> hierarchies 2. For the fast creation of hot new newsgroups\\nLiefting> like alt.gulf.war\\n\\nLiefting> Because there is no voting process or any other control\\nLiefting> facilities, sites are free to decide not to carry (some of)\\nLiefting> the alt groups.\\n\\nLiefting> Therefore, it is (I think) desirable to try to create\\nLiefting> comp.graphics.  {raytrace, rendering or whatever} and not an\\nLiefting> alt-group\\n\\nPlus, *many* sites, (especially many .com sites) do not carry any alt\\nnewsgroups. (We don\\'t for example.) A comp.* group will get a much broader\\ndistribution and would be useful to many more people. Plus the topic is\\nimportant/popular enough to warrant its own group, IMHO.\\n--\\n/**************************************************************************/\\n/* Kent Dalton                   * EMail: Kent.Dalton@FtCollinsCO.NCR.COM */\\n/* NCR Microelectronics          * Phone: (303) 223-5100 X-319            */  \\n/* 2001 Danfield Ct. MS470A      *   FAX: (303) 226-9556                  */\\n/* Fort Collins, Colorado 80525  *                                        */\\n/**************************************************************************/\\nDoes someone from PEORIA have a SHORTER ATTENTION span than me?\\n\\nThe correct answer would be: Technology\\nFor the following text: \\nFrom: cookson@mbunix.mitre.org (Cookson)\\nSubject: Re: Why I wanted police officers to answer my posting\\nNntp-Posting-Host: mbunix.mitre.org\\nOrganization: The MITRE Corporation, Bedford, MA\\nLines: 29\\n\\nIn article <viking.735733789@ponderous.cc.iastate.edu> viking@iastate.edu (Dan Sorenson) writes:\\n>\\n>\\tHOW TO GET A VERBAL WARNING FOR 146 IN A 55\\n>\\n\\nPoppy cock!  This story is obviously a complete fabrication, and I\\'ll show\\nyou why...\\n\\n>\\tIn Ames, there is a road that leads to the little town of\\n           ^^^^\\nThis establishes that the story takes place in Iowa.\\n>Gilbert.  Gilbert has one stop light, if that tells you something.\\n>Having just gotten the bike back together, I thought I\\'d take it\\n>for a short ride and check things out.  Heading out of town, I\\n>went into the twisties at a slow pace, just under the speed limit,\\n               ^^^^^^^^\\nIn Iowa?!?!?  Come on now Dan, how dumb do you think we are?  You could\\nhave at least thrown in a llama or tennis ball reference.  Hell, you\\ndidn\\'t even get the speed right.\\n\\nDean\\n\\nps. :-)\\n\\n-- \\n| Dean Cookson / dcookson@mitre.org / 617 271-2714    | DoD #207  AMA #573534 |\\n| The MITRE Corp. Burlington Rd., Bedford, Ma. 01730  | KotNML  /  KotB       |\\n| \"The road is my shepherd and I shall not stop\"      | \\'92 VFR750F           |\\n| -Sam Eliott, Road Hogs MTV 1993                     | \\'88 Bianchi Limited   |\\n\\nThe correct answer would be: Vehicles\\nNow the text, you have to classify will follow. Please assess its topic and answer only the topic of it.\\nFrom: gloster@Inference.COM (Vance M. Gloster)\\nSubject: Re: Compositing pictures on PC?\\nOrganization: Inference Corporation\\nLines: 28\\nNNTP-Posting-Host: fourier.inference.com\\nIn-reply-to: chu@TorreyPinesCA.ncr.com\\'s message of Sat, 15 May 93 00:16:31 GMT\\n\\nIn article <1993May15.001631.7051@TorreyPinesCA.ncr.com> chu@TorreyPinesCA.ncr.com (Patrick Chu 3605) writes:\\n\\n   I was wondering if anyone knows of a graphics package for the PC that\\n   will do compositing of a series of pictures?\\n\\n   What I mean by \"compositing\" is, say I have a live video clip\\n   (digitized) panning around a living room, and a computer-generated\\n   bird flying around the screen.  I want to combine these two series of\\n   pictures so that everywhere where the bird frames are black, I want\\n   the living room picture to show through.  Yes, I realize I can do this\\n   with a genlock, and I do own a genlock, but I want to be able to do\\n   manual compositing also.  It\\'s ok if I have to composite one frame at\\n   a time; I assumed I\\'d have to do that anyway.  But being able to\\n   composite a series of frames would be even better.\\n\\n   I\\'ve looked around and I haven\\'t found a PC package that will perform\\n   this.  Help, please!\\n\\nIf you can get the live animation and the computer-generated animation\\ninto AutoDesk Animator .FLI or .FLC format, AutoDesk Animator will do\\nthis for you.  It can take one animation, make a certain color\\n\"clear\", and overlay it over another animation.  I do not have a way\\nright now to convert .AVI or .MPG files to animator files.  Animator\\nwill also import a series of .GIF files to create an animation, so if\\nyour video capture stuff can create this is might work.\\n\\n-Vance Gloster\\n gloster@inference.com\\n\\n'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def make_prompts(bbc_instructions, bbc_data):\n",
    "    prompts = []\n",
    "    \n",
    "    # General instructions and fixed texts\n",
    "    general_instruction = (\n",
    "        \"You are a perfect topic modeling machine. Given a text and the different topics, \"\n",
    "        \"you will classify the texts to the correct topic. First you will receive the topics, \"\n",
    "        \"afterwards an example and finally the text you have to assign one of the before mentioned topics to.\"\n",
    "    )\n",
    "    topics = \"The topics are sport, technology, politics, vehicles, religion, miscellaneous and science. Please make sure, you know the topics and their meaning.\"\n",
    "    transition_to_examples = \"Now an example for each of the categories will follow.\"\n",
    "    transition_to_text_to_classify = (\n",
    "        \"Now the text, you have to classify will follow. Please assess its topic and answer only the topic of it.\"\n",
    "    )\n",
    "\n",
    "    # Iterate through the test DataFrame rows\n",
    "    for _, test_row in bbc_data.iterrows():\n",
    "        prompt = general_instruction + \"\\n\" + topics + \"\\n\" + transition_to_examples + \"\\n\"\n",
    "\n",
    "        # Iterate through instruction DataFrame to add examples\n",
    "        for _, instruction_row in bbc_instructions.iterrows():\n",
    "            category = instruction_row['label']\n",
    "            example_text = instruction_row['text']\n",
    "            prompt += f\"For the following text: \\n{example_text}\\nThe correct answer would be: {category}\\n\"\n",
    "\n",
    "        # Add the actual text to classify from the test set\n",
    "        text_to_classify = test_row['text']\n",
    "        prompt += transition_to_text_to_classify + \"\\n\" + text_to_classify + \"\\n\"\n",
    "        name = f\"{test_row['name']}\"\n",
    "        #print(name)\n",
    "        prompt_dict = {}\n",
    "        prompt_dict[name] = prompt\n",
    "        prompts.append(prompt_dict)\n",
    "    return prompts            \n",
    "\n",
    "prompts = make_prompts(instruction_df, test_df)\n",
    "prompts = prompts[:1]\n",
    "print(prompts[0])\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pipeline cannot infer suitable model classes from meta-llama/Llama-3.2-1B",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_llama\u001b[39m(prompt):\n\u001b[0;32m      8\u001b[0m     semaphore \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mSemaphore(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:896\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 896\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    907\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m     class_tuple \u001b[38;5;241m=\u001b[39m class_tuple \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(classes)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_tuple) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline cannot infer suitable model classes from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    271\u001b[0m all_traceback \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_class \u001b[38;5;129;01min\u001b[39;00m class_tuple:\n",
      "\u001b[1;31mValueError\u001b[0m: Pipeline cannot infer suitable model classes from meta-llama/Llama-3.2-1B"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-3.2-1B\",\n",
    "    framework=\"torch\",\n",
    ")\n",
    "\n",
    "async def prompt_llama(prompt):\n",
    "    semaphore = asyncio.Semaphore(5)\n",
    "    async with semaphore:\n",
    "\n",
    "        name = list(prompt.keys())[0]\n",
    "        promptAI = prompt[name]\n",
    "\n",
    "        response = await asyncio.to_thread(pipe, promptAI, max_length=100)\n",
    "\n",
    "        text_response = response[0][\"generated_text\"]\n",
    "\n",
    "        print(f\"Response:\", text_response)\n",
    "        return {name: text_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, LlamaForCausalLM\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-3.2-1B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitialized \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   4007\u001b[0m     (\n\u001b[0;32m   4008\u001b[0m         model,\n\u001b[0;32m   4009\u001b[0m         missing_keys,\n\u001b[0;32m   4010\u001b[0m         unexpected_keys,\n\u001b[0;32m   4011\u001b[0m         mismatched_keys,\n\u001b[0;32m   4012\u001b[0m         offload_index,\n\u001b[0;32m   4013\u001b[0m         error_msgs,\n\u001b[1;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:4447\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[0;32m   4442\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4443\u001b[0m         \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n\u001b[0;32m   4444\u001b[0m         assign_to_params_buffers \u001b[38;5;241m=\u001b[39m check_support_param_buffer_assignment(\n\u001b[0;32m   4445\u001b[0m             model_to_load, state_dict, start_prefix\n\u001b[0;32m   4446\u001b[0m         )\n\u001b[1;32m-> 4447\u001b[0m         error_msgs \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\n\u001b[0;32m   4449\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4452\u001b[0m     \u001b[38;5;66;03m# This should always be a list but, just to be sure.\u001b[39;00m\n\u001b[0;32m   4453\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:761\u001b[0m, in \u001b[0;36m_load_state_dict_into_model\u001b[1;34m(model_to_load, state_dict, start_prefix, assign_to_params_buffers)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    759\u001b[0m             load(child, state_dict, prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, assign_to_params_buffers)\n\u001b[1;32m--> 761\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;66;03m# Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;66;03m# it's safe to delete it.\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m state_dict\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:759\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 759\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:759\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 759\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: _load_state_dict_into_model.<locals>.load at line 759 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:759\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 759\u001b[0m         \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massign_to_params_buffers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:755\u001b[0m, in \u001b[0;36m_load_state_dict_into_model.<locals>.load\u001b[1;34m(module, state_dict, prefix, assign_to_params_buffers)\u001b[0m\n\u001b[0;32m    753\u001b[0m                     module\u001b[38;5;241m.\u001b[39m_load_from_state_dict(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 755\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\elias\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2441\u001b[0m, in \u001b[0;36mModule._load_from_state_dict\u001b[1;34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[0m\n\u001b[0;32m   2439\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, input_param)\n\u001b[0;32m   2440\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2441\u001b[0m             \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_param\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m   2443\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswapping\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_swap_tensors \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopying\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "print(\"initialized \")\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(\"tokenized \")\n",
    "# Generate\n",
    "generate_ids = model.generate(inputs, max_length=30)\n",
    "print(\"generated \")\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_correct_anser(results, test_df):\n",
    "    result_with_correct_answer = []\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            value = value.replace(\"\\n\", \"\")\n",
    "            value = value.replace(\" \\n\", \"\")\n",
    "            value = value.replace(\" \",\"\")\n",
    "            value = value.replace(\"*\",\"\")\n",
    "            value = value.lower()\n",
    "            matching_rows = test_df.loc[test_df['name'] == int(key), 'label']\n",
    "            if not matching_rows.empty:\n",
    "                if type(matching_rows.values[0]) == float: \n",
    "                    print(f\"Nan value\", matching_rows.values[0])\n",
    "                else:\n",
    "                    category_value = matching_rows.values[0]\n",
    "                    category_value = category_value.lower()\n",
    "                    result_with_correct_answer.append({key: (value, category_value)})\n",
    "            else:\n",
    "                print(f\"No matching category found for key: {key}\")\n",
    "                result_with_correct_answer.append({key: (value, None)})\n",
    "    \n",
    "    return result_with_correct_answer\n",
    "results_with_correct_answer = add_correct_anser(results, test_df)\n",
    "print(len(results_with_correct_answer))\n",
    "print(results_with_correct_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ground_truth_and_predictions(results_with_correct_answer):\n",
    "    ground_truth = []\n",
    "    predictions = []\n",
    "    for result in results_with_correct_answer:\n",
    "        for key, value in result.items():\n",
    "            ground_truth.append(value[1])\n",
    "            predictions.append(value[0])\n",
    "    return ground_truth, predictions\n",
    "ground_truth, predictions = extract_ground_truth_and_predictions(results_with_correct_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NMI(ground_truth, predictions):\n",
    "    \n",
    "    nmi_score = normalized_mutual_info_score(ground_truth, predictions)\n",
    "    print(f\"Normalized Mutual Information Score: {nmi_score}\")\n",
    "    return nmi_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_purity(predicted_labels, true_labels):\n",
    "    # Convert lists to numpy arrays for easier indexing\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Get unique clusters\n",
    "    unique_clusters = np.unique(predicted_labels)\n",
    "    \n",
    "    # Total number of instances\n",
    "    total_instances = len(true_labels)\n",
    "    \n",
    "    # Calculate the number of correctly classified instances in each cluster\n",
    "    correctly_classified = 0\n",
    "    for cluster in unique_clusters:\n",
    "        # Get the true labels of instances in the current cluster\n",
    "        indices_in_cluster = np.where(predicted_labels == cluster)[0]\n",
    "        labels_in_cluster = true_labels[indices_in_cluster]\n",
    "        \n",
    "        # Determine the most common true label in this cluster\n",
    "        majority_label_count = Counter(labels_in_cluster).most_common(1)[0][1]\n",
    "        \n",
    "        # Add the number of correctly classified instances in this cluster\n",
    "        correctly_classified += majority_label_count\n",
    "    \n",
    "    # Calculate purity\n",
    "    purity = correctly_classified / total_instances\n",
    "    print(f\"Purity: {purity}\")\n",
    "    return purity\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted_labels, true_labels):\n",
    "    # Ensure that the predicted_labels and true_labels have the same length\n",
    "    if len(predicted_labels) != len(true_labels):\n",
    "        raise ValueError(\"The length of predicted and true labels must be the same.\")\n",
    "    \n",
    "    # Count the number of correct predictions\n",
    "    correct_predictions = sum(1 for pred, true in zip(predicted_labels, true_labels) if pred == true)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / len(true_labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def calculate_f1_score(ground_truth, predictions):\n",
    "\n",
    "    f1 = f1_score(ground_truth, predictions, average='micro') # Because there might be over/ under representation of some classes\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_NMI(ground_truth, predictions)\n",
    "calculate_purity(predictions, ground_truth)\n",
    "calculate_accuracy(predictions, ground_truth)\n",
    "calculate_f1_score(ground_truth, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
