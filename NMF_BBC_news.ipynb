{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from octis.models.NMF import NMF\n",
    "from octis.dataset.dataset import Dataset\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "from sklearn import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BBC News dataset\n",
    "bbc_news = pd.read_csv(\"/home/patsias/Essential Text/Comparing-Different-Topic-Modeling-Methods-on-News/bbc-news-data.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  \n",
    "    words = text.split()  \n",
    "    words = [word for word in words if word.lower() not in stop_words]  \n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/patsias/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Combine 'title' and 'content' into a 'text' column\n",
    "bbc_news['text'] = bbc_news['title'] + \" \" + bbc_news['content']\n",
    "\n",
    "# Apply the preprocessing function\n",
    "bbc_news['processed_text'] = bbc_news['text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the text into sets of words\n",
    "bbc_news['tok'] = bbc_news['processed_text'].apply(lambda x: set(x.split()))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train, test = train_test_split(bbc_news, test_size=0.2, random_state=42)\n",
    "train['div'] = 'train'\n",
    "test['div'] = 'test'\n",
    "\n",
    "# Merge back train and test into one dataframe\n",
    "bbc_news_split = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# Create a Gensim dictionary for tokenized words\n",
    "train_docs = bbc_news_split[bbc_news_split['div'] == 'train'].tok.to_numpy()\n",
    "dictionary = gensim.corpora.Dictionary(train_docs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>filename</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>tok</th>\n",
       "      <th>div</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sport</td>\n",
       "      <td>178.txt</td>\n",
       "      <td>Cole refuses to blame van Persie</td>\n",
       "      <td>Ashley Cole has refused to blame Robin van Pe...</td>\n",
       "      <td>Cole refuses to blame van Persie  Ashley Cole ...</td>\n",
       "      <td>cole refuses blame van persie ashley cole refu...</td>\n",
       "      <td>{leaving, ljungberg, marys, sprain, refused, c...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tech</td>\n",
       "      <td>178.txt</td>\n",
       "      <td>Slimmer PlayStation triple sales</td>\n",
       "      <td>Sony PlayStation 2's slimmer shape has proved...</td>\n",
       "      <td>Slimmer PlayStation triple sales  Sony PlaySta...</td>\n",
       "      <td>slimmer playstation triple sales sony playstat...</td>\n",
       "      <td>{really, original, battlefield, may, broke, sl...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>260.txt</td>\n",
       "      <td>Bellamy fined after row</td>\n",
       "      <td>Newcastle have fined their Welsh striker Crai...</td>\n",
       "      <td>Bellamy fined after row  Newcastle have fined ...</td>\n",
       "      <td>bellamy fined row newcastle fined welsh strike...</td>\n",
       "      <td>{admitted, refused, game, arsenal, theres, apo...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>017.txt</td>\n",
       "      <td>Finding new homes for old phones</td>\n",
       "      <td>Re-using old mobile phones is not just good f...</td>\n",
       "      <td>Finding new homes for old phones  Re-using old...</td>\n",
       "      <td>finding new homes old phones reusing old mobil...</td>\n",
       "      <td>{really, buy, enjoy, friends, environmental, h...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>101.txt</td>\n",
       "      <td>Sundance to honour foreign films</td>\n",
       "      <td>International films will be given the same pr...</td>\n",
       "      <td>Sundance to honour foreign films  Internationa...</td>\n",
       "      <td>sundance honour foreign films international fi...</td>\n",
       "      <td>{independent, korea, carruth, china, include, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>232.txt</td>\n",
       "      <td>Connick Jr to lead Broadway show</td>\n",
       "      <td>Singer and actor Harry Connick Jr is to star ...</td>\n",
       "      <td>Connick Jr to lead Broadway show  Singer and a...</td>\n",
       "      <td>connick jr lead broadway show singer actor har...</td>\n",
       "      <td>{original, ice, game, age, steam, wrote, raqui...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>business</td>\n",
       "      <td>206.txt</td>\n",
       "      <td>Standard Life cuts policy bonuses</td>\n",
       "      <td>Standard Life, Europe's largest mutual life i...</td>\n",
       "      <td>Standard Life cuts policy bonuses  Standard Li...</td>\n",
       "      <td>standard life cuts policy bonuses standard lif...</td>\n",
       "      <td>{time, pensions, withprofits, sticking, payout...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>politics</td>\n",
       "      <td>207.txt</td>\n",
       "      <td>February poll claim 'speculation'</td>\n",
       "      <td>Reports that Tony Blair is planning a snap ge...</td>\n",
       "      <td>February poll claim 'speculation'  Reports tha...</td>\n",
       "      <td>february poll claim speculation reports tony b...</td>\n",
       "      <td>{may, ultimately, around, include, government,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>159.txt</td>\n",
       "      <td>Band Aid 20 single storms to No 1</td>\n",
       "      <td>The new version of the Band Aid song Do They ...</td>\n",
       "      <td>Band Aid 20 single storms to No 1  The new ver...</td>\n",
       "      <td>band aid 20 single storms 1 new version band a...</td>\n",
       "      <td>{original, grammy, chart, dido, website, singl...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>business</td>\n",
       "      <td>480.txt</td>\n",
       "      <td>Wipro beats forecasts once again</td>\n",
       "      <td>Wipro, India's third-biggest software firm, h...</td>\n",
       "      <td>Wipro beats forecasts once again  Wipro, India...</td>\n",
       "      <td>wipro beats forecasts wipro indias thirdbigges...</td>\n",
       "      <td>{34, staff, owned, problem, clients, centres, ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category filename                              title  \\\n",
       "0             sport  178.txt   Cole refuses to blame van Persie   \n",
       "1              tech  178.txt   Slimmer PlayStation triple sales   \n",
       "2             sport  260.txt            Bellamy fined after row   \n",
       "3              tech  017.txt   Finding new homes for old phones   \n",
       "4     entertainment  101.txt   Sundance to honour foreign films   \n",
       "...             ...      ...                                ...   \n",
       "2220  entertainment  232.txt   Connick Jr to lead Broadway show   \n",
       "2221       business  206.txt  Standard Life cuts policy bonuses   \n",
       "2222       politics  207.txt  February poll claim 'speculation'   \n",
       "2223  entertainment  159.txt  Band Aid 20 single storms to No 1   \n",
       "2224       business  480.txt   Wipro beats forecasts once again   \n",
       "\n",
       "                                                content  \\\n",
       "0      Ashley Cole has refused to blame Robin van Pe...   \n",
       "1      Sony PlayStation 2's slimmer shape has proved...   \n",
       "2      Newcastle have fined their Welsh striker Crai...   \n",
       "3      Re-using old mobile phones is not just good f...   \n",
       "4      International films will be given the same pr...   \n",
       "...                                                 ...   \n",
       "2220   Singer and actor Harry Connick Jr is to star ...   \n",
       "2221   Standard Life, Europe's largest mutual life i...   \n",
       "2222   Reports that Tony Blair is planning a snap ge...   \n",
       "2223   The new version of the Band Aid song Do They ...   \n",
       "2224   Wipro, India's third-biggest software firm, h...   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Cole refuses to blame van Persie  Ashley Cole ...   \n",
       "1     Slimmer PlayStation triple sales  Sony PlaySta...   \n",
       "2     Bellamy fined after row  Newcastle have fined ...   \n",
       "3     Finding new homes for old phones  Re-using old...   \n",
       "4     Sundance to honour foreign films  Internationa...   \n",
       "...                                                 ...   \n",
       "2220  Connick Jr to lead Broadway show  Singer and a...   \n",
       "2221  Standard Life cuts policy bonuses  Standard Li...   \n",
       "2222  February poll claim 'speculation'  Reports tha...   \n",
       "2223  Band Aid 20 single storms to No 1  The new ver...   \n",
       "2224  Wipro beats forecasts once again  Wipro, India...   \n",
       "\n",
       "                                         processed_text  \\\n",
       "0     cole refuses blame van persie ashley cole refu...   \n",
       "1     slimmer playstation triple sales sony playstat...   \n",
       "2     bellamy fined row newcastle fined welsh strike...   \n",
       "3     finding new homes old phones reusing old mobil...   \n",
       "4     sundance honour foreign films international fi...   \n",
       "...                                                 ...   \n",
       "2220  connick jr lead broadway show singer actor har...   \n",
       "2221  standard life cuts policy bonuses standard lif...   \n",
       "2222  february poll claim speculation reports tony b...   \n",
       "2223  band aid 20 single storms 1 new version band a...   \n",
       "2224  wipro beats forecasts wipro indias thirdbigges...   \n",
       "\n",
       "                                                    tok    div  \n",
       "0     {leaving, ljungberg, marys, sprain, refused, c...  train  \n",
       "1     {really, original, battlefield, may, broke, sl...  train  \n",
       "2     {admitted, refused, game, arsenal, theres, apo...  train  \n",
       "3     {really, buy, enjoy, friends, environmental, h...  train  \n",
       "4     {independent, korea, carruth, china, include, ...  train  \n",
       "...                                                 ...    ...  \n",
       "2220  {original, ice, game, age, steam, wrote, raqui...   test  \n",
       "2221  {time, pensions, withprofits, sticking, payout...   test  \n",
       "2222  {may, ultimately, around, include, government,...   test  \n",
       "2223  {original, grammy, chart, dido, website, singl...   test  \n",
       "2224  {34, staff, owned, problem, clients, centres, ...   test  \n",
       "\n",
       "[2225 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_news_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.json has been successfully created.\n"
     ]
    }
   ],
   "source": [
    "# Create the directory first\n",
    "os.makedirs('bbc_octis', exist_ok=True)\n",
    "\n",
    "# Save the vocabulary (for OCTIS)\n",
    "vocab_length = len(dictionary)\n",
    "with open(\"bbc_octis/vocabulary.txt\", \"w\", encoding='utf8') as f:\n",
    "    for i in range(vocab_length):\n",
    "        f.write(dictionary[i] + '\\n')\n",
    "\n",
    "# Create Bag-of-Words (BoW) representation\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in bbc_news_split['tok']]\n",
    "bbc_news_split['corpus'] = bow_corpus\n",
    "\n",
    "# Save the corpus as 'corpus.tsv'\n",
    "bbc_news_split[['processed_text', 'div', 'category']].to_csv(\"bbc_octis/corpus.tsv\", sep='\\t', index=False, header=False)\n",
    "\n",
    "# Create metadata.json for categories\n",
    "# Extract labels from your dataframe\n",
    "labels = bbc_news_split['category'].tolist()\n",
    "\n",
    "# Write labels to metadata.json\n",
    "metadata = {\"labels\": labels}\n",
    "with open(\"bbc_octis/metadata.json\", \"w\") as outfile:\n",
    "    json.dump(metadata, outfile)\n",
    "\n",
    "print(\"metadata.json has been successfully created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for purity and normalized mutual information (NMI)\n",
    "def q_metrics(y_true, y_pred,my_model=None):\n",
    "    contigency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    purity = np.sum(np.amax(contigency_matrix, axis=0)) / np.sum(contigency_matrix)\n",
    "    print('Purity Score:', purity)\n",
    "    print('NMI:', metrics.normalized_mutual_info_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity Score: 0.40224719101123596\n",
      "NMI: 0.16238347180567467\n",
      "Coherence Score: 0.4912102512292873\n",
      "w_max_iter : 50 ; kappa : 1.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.4044943820224719\n",
      "NMI: 0.08961338414097103\n",
      "Coherence Score: 0.531511401881635\n",
      "w_max_iter : 50 ; kappa : 2.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.27415730337078653\n",
      "NMI: 0.009434547618065304\n",
      "Coherence Score: 0.34809037467731574\n",
      "w_max_iter : 50 ; kappa : 3.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.5258426966292135\n",
      "NMI: 0.2930929034349417\n",
      "Coherence Score: 0.41293875270369584\n",
      "w_max_iter : 50 ; kappa : 1.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.36179775280898874\n",
      "NMI: 0.08367728613870756\n",
      "Coherence Score: 0.5297993785859021\n",
      "w_max_iter : 50 ; kappa : 2.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.27415730337078653\n",
      "NMI: 0.013832161458728886\n",
      "Coherence Score: 0.3489134118838092\n",
      "w_max_iter : 50 ; kappa : 3.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.597752808988764\n",
      "NMI: 0.3351795054673556\n",
      "Coherence Score: 0.5050475854559323\n",
      "w_max_iter : 50 ; kappa : 1.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.34831460674157305\n",
      "NMI: 0.07720011082041994\n",
      "Coherence Score: 0.5728647728215188\n",
      "w_max_iter : 50 ; kappa : 2.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.298876404494382\n",
      "NMI: 0.01790914084523072\n",
      "Coherence Score: 0.3436288761127594\n",
      "w_max_iter : 50 ; kappa : 3.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.7393258426966293\n",
      "NMI: 0.48090480419516685\n",
      "Coherence Score: 0.5666135441835983\n",
      "w_max_iter : 100 ; kappa : 1.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.35730337078651686\n",
      "NMI: 0.07084944786481306\n",
      "Coherence Score: 0.5602830836061838\n",
      "w_max_iter : 100 ; kappa : 2.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.28314606741573034\n",
      "NMI: 0.01959780823822467\n",
      "Coherence Score: 0.4076870857141226\n",
      "w_max_iter : 100 ; kappa : 3.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.6584269662921348\n",
      "NMI: 0.3838244575279512\n",
      "Coherence Score: 0.5520896933305084\n",
      "w_max_iter : 100 ; kappa : 1.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.41123595505617977\n",
      "NMI: 0.0938722948237728\n",
      "Coherence Score: 0.5466272169363449\n",
      "w_max_iter : 100 ; kappa : 2.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.2966292134831461\n",
      "NMI: 0.015189197719409662\n",
      "Coherence Score: 0.3468385708940581\n",
      "w_max_iter : 100 ; kappa : 3.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.5438202247191011\n",
      "NMI: 0.3017138204576115\n",
      "Coherence Score: 0.528216018185851\n",
      "w_max_iter : 100 ; kappa : 1.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.3865168539325843\n",
      "NMI: 0.06662611179465312\n",
      "Coherence Score: 0.48262639798835655\n",
      "w_max_iter : 100 ; kappa : 2.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.2876404494382023\n",
      "NMI: 0.013678265299887932\n",
      "Coherence Score: 0.3508397780135383\n",
      "w_max_iter : 100 ; kappa : 3.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.6\n",
      "NMI: 0.36214127401995316\n",
      "Coherence Score: 0.5414966394817495\n",
      "w_max_iter : 200 ; kappa : 1.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.30337078651685395\n",
      "NMI: 0.029952988023895902\n",
      "Coherence Score: 0.4742819384540817\n",
      "w_max_iter : 200 ; kappa : 2.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.2651685393258427\n",
      "NMI: 0.010446714739884016\n",
      "Coherence Score: 0.3480903746773158\n",
      "w_max_iter : 200 ; kappa : 3.0 ; h_max_iter : 50 \n",
      "Purity Score: 0.4\n",
      "NMI: 0.19016586627124088\n",
      "Coherence Score: 0.4547741489091819\n",
      "w_max_iter : 200 ; kappa : 1.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.3685393258426966\n",
      "NMI: 0.0708307943633257\n",
      "Coherence Score: 0.4661223319724194\n",
      "w_max_iter : 200 ; kappa : 2.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.28314606741573034\n",
      "NMI: 0.01347431708453397\n",
      "Coherence Score: 0.3497364490903027\n",
      "w_max_iter : 200 ; kappa : 3.0 ; h_max_iter : 100 \n",
      "Purity Score: 0.5370786516853933\n",
      "NMI: 0.29523953510111545\n",
      "Coherence Score: 0.4615549093927873\n",
      "w_max_iter : 200 ; kappa : 1.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.3842696629213483\n",
      "NMI: 0.08080699415249952\n",
      "Coherence Score: 0.5599049615746321\n",
      "w_max_iter : 200 ; kappa : 2.0 ; h_max_iter : 200 \n",
      "Purity Score: 0.27415730337078653\n",
      "NMI: 0.008581921283395197\n",
      "Coherence Score: 0.3443231717762402\n",
      "w_max_iter : 200 ; kappa : 3.0 ; h_max_iter : 200 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset prepared for OCTIS\n",
    "bbc_dataset = Dataset()\n",
    "bbc_dataset.load_custom_dataset_from_folder('bbc_octis')\n",
    "\n",
    "# Define and train NMF model\n",
    "def calculate_coherence_score(num_topics, kappa, w_max_iter, h_max_iter):\n",
    "    model = NMF(num_topics=num_topics, chunksize=2000, passes=10, kappa=kappa,\n",
    "                minimum_probability=0.01, w_max_iter=w_max_iter,\n",
    "                w_stop_condition=0.0001, h_max_iter=h_max_iter, h_stop_condition=0.001,\n",
    "                eval_every=10, normalize=True)\n",
    "\n",
    "    # Train the model\n",
    "    nmf_output = model.train_model(bbc_dataset)\n",
    "\n",
    "    # Get the test set results (document-topic matrix)\n",
    "    test_res = nmf_output['test-topic-document-matrix'].T\n",
    "    pred = [np.argmax(res) for res in test_res]\n",
    "\n",
    "    # Load true labels from the dataset\n",
    "    df = pd.read_csv(\"bbc_octis/corpus.tsv\", sep='\\t', header=None)\n",
    "    y_true = df[df[1] == 'test'][2].values  # Assuming 2nd column is 'category'\n",
    "\n",
    "    # Evaluate metrics\n",
    "    q_metrics(y_true, pred)\n",
    "\n",
    "    # # Calculate Coherence\n",
    "    coherence = Coherence(texts=bbc_dataset.get_corpus(), topk=10, measure='c_v')\n",
    "    coherence_score = coherence.score(nmf_output)\n",
    "    print(f\"Coherence Score: {coherence_score}\")\n",
    "\n",
    "    return coherence_score\n",
    "\n",
    "#list containing various hyperparameters\n",
    "kappa = [1.0, 2.0,3.0]\n",
    "w_max_iter = [50, 100, 200]\n",
    "h_max_iter = [50, 100, 200]\n",
    "\n",
    "\n",
    "for w in w_max_iter:\n",
    "    for h in h_max_iter:\n",
    "        for k in kappa:\n",
    "            calculate_coherence_score(num_topics=5, kappa=k, w_max_iter=w, h_max_iter=h)   # Coherence measures how interpretable the topics generated by the model are. A higher coherence score generally means better, more interpretable topics.\n",
    "            print(f\"w_max_iter : {w} ; kappa : {k} ; h_max_iter : {h} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity Score: 0.6\n",
      "NMI: 0.33122808347575877\n",
      "Coherence Score: 0.5169698059202658\n"
     ]
    }
   ],
   "source": [
    "w_max_iter : 100 ; kappa : 1.0 ; h_max_iter : 50\n",
    "model=NMF(num_topics=5, chunksize=2000, passes=10, kappa=1.0,\n",
    "                minimum_probability=0.01, w_max_iter=100,\n",
    "                w_stop_condition=0.0001, h_max_iter=50, h_stop_condition=0.001,\n",
    "                eval_every=10, normalize=True)\n",
    "\n",
    "bbc_dataset = Dataset()\n",
    "bbc_dataset.load_custom_dataset_from_folder('bbc_octis')\n",
    "nmf_output = model.train_model(bbc_dataset)\n",
    "\n",
    "test_res = nmf_output['test-topic-document-matrix'].T\n",
    "pred = [np.argmax(res) for res in test_res]\n",
    "\n",
    "# Load true labels from the dataset\n",
    "df = pd.read_csv(\"bbc_octis/corpus.tsv\", sep='\\t', header=None)\n",
    "y_true = df[df[1] == 'test'][2].values  # Assuming 2nd column is 'category'\n",
    "\n",
    "y_pred = pred\n",
    "q_metrics(y_true, y_pred)\n",
    "\n",
    "\n",
    "# evaluate model using Topic Coherence score\n",
    "coherence = Coherence(texts=bbc_dataset.get_corpus(), topk=10, measure='c_v')\n",
    "coherence_score = coherence.score(nmf_output)\n",
    "print(f\"Coherence Score: {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t Counter({'sport': 81, 'entertainment': 19, 'tech': 9, 'business': 3, 'politics': 1})\n",
      "1 \t Counter({'tech': 54, 'business': 28, 'entertainment': 8, 'politics': 6})\n",
      "2 \t Counter({'politics': 42, 'business': 13, 'entertainment': 4, 'tech': 4})\n",
      "3 \t Counter({'business': 71, 'politics': 27, 'entertainment': 22, 'sport': 20, 'tech': 12})\n",
      "4 \t Counter({'entertainment': 19, 'tech': 1, 'sport': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "test_res = nmf_output['test-topic-document-matrix'].T\n",
    "pred = [np.argmax(res) for res in test_res]\n",
    "    \n",
    "temp = pd.DataFrame()\n",
    "temp['y_true'] = y_true\n",
    "temp['y_pred'] = pred\n",
    "for i in range(5):\n",
    "    print(i,'\\t',Counter(temp[temp['y_pred']==i]['y_true']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cole refuses blame van persie ashley cole refu...</td>\n",
       "      <td>train</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>slimmer playstation triple sales sony playstat...</td>\n",
       "      <td>train</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bellamy fined row newcastle fined welsh strike...</td>\n",
       "      <td>train</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>finding new homes old phones reusing old mobil...</td>\n",
       "      <td>train</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sundance honour foreign films international fi...</td>\n",
       "      <td>train</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>connick jr lead broadway show singer actor har...</td>\n",
       "      <td>test</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>standard life cuts policy bonuses standard lif...</td>\n",
       "      <td>test</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>february poll claim speculation reports tony b...</td>\n",
       "      <td>test</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>band aid 20 single storms 1 new version band a...</td>\n",
       "      <td>test</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>wipro beats forecasts wipro indias thirdbigges...</td>\n",
       "      <td>test</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0      1              2\n",
       "0     cole refuses blame van persie ashley cole refu...  train          sport\n",
       "1     slimmer playstation triple sales sony playstat...  train           tech\n",
       "2     bellamy fined row newcastle fined welsh strike...  train          sport\n",
       "3     finding new homes old phones reusing old mobil...  train           tech\n",
       "4     sundance honour foreign films international fi...  train  entertainment\n",
       "...                                                 ...    ...            ...\n",
       "2220  connick jr lead broadway show singer actor har...   test  entertainment\n",
       "2221  standard life cuts policy bonuses standard lif...   test       business\n",
       "2222  february poll claim speculation reports tony b...   test       politics\n",
       "2223  band aid 20 single storms 1 new version band a...   test  entertainment\n",
       "2224  wipro beats forecasts wipro indias thirdbigges...   test       business\n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity Score: 0.6\n",
      "NMI: 0.33122808347575877\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.47      0.62      0.53       115\n",
      "entertainment       0.90      0.26      0.41        72\n",
      "     politics       0.67      0.55      0.60        76\n",
      "        sport       0.72      0.79      0.75       102\n",
      "         tech       0.56      0.68      0.61        80\n",
      "\n",
      "     accuracy                           0.60       445\n",
      "    macro avg       0.66      0.58      0.58       445\n",
      " weighted avg       0.65      0.60      0.59       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "topic_name={0:'sport',1:'tech',2:'politics',3:'business',\n",
    "            4:'entertainment'}\n",
    "\n",
    "y_true = df[df[1]=='test'][2].to_list()\n",
    "y_pred = [*map(topic_name.get, pred)]\n",
    "\n",
    "q_metrics(y_true, pred)\n",
    "print(classification_report(y_true,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
