{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name       label  result\n",
      "0      1.0  Technology     NaN\n",
      "1      2.0     Science     NaN\n",
      "2      3.0     Science     NaN\n",
      "3      4.0     Science     NaN\n",
      "4      5.0    Politics     NaN\n",
      "..     ...         ...     ...\n",
      "888  889.0       Sport     NaN\n",
      "889  890.0    Vehicles     NaN\n",
      "890  891.0    Politics     NaN\n",
      "891  892.0       Sport     NaN\n",
      "892  893.0     Science     NaN\n",
      "\n",
      "[893 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'E:\\uni\\NLP group project\\originals\\gemini_7newsgroups.csv')\n",
    "df = df[['Unnamed: 0', 'text', 'label20', 'labels_name20', 'label', 'div', 'name', 'result']]\n",
    "df = df[['name','label', 'result']]\n",
    "df[['label', 'result']] = df[['label', 'result']].apply(lambda x: x.str.lower())\n",
    "df = df.iloc[:-1]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def post_process(df, results):\n",
    "      # Ensure the DataFrame has a 'name' and 'result' column\n",
    "      string_list = [\"religion\", \"politics\", \"sport\", \"technology\", \"vehicles\", \"science\", \"miscellaneous\"]\n",
    "        \n",
    "      # Iterate through each result in the results list\n",
    "      for result in results:\n",
    "            similarity = []\n",
    "            for string in string_list:\n",
    "                  # Calculate similarity score using fuzz.partial_ratio\n",
    "                  similarity.append([fuzz.partial_ratio(list(result.values())[0][0], string), string])\n",
    "            \n",
    "            # Get the string with the highest similarity score\n",
    "            max_similarity = max(similarity)\n",
    "            \n",
    "            # If similarity score is greater than 60, update result in DataFrame\n",
    "            if max_similarity[0] > 60:\n",
    "                  df.loc[df['name'] == list(result.keys())[0], 'result'] = max_similarity[1]\n",
    "                  \n",
    "            else:\n",
    "                  # If no strong match, set result to an empty string\n",
    "                  df.loc[df['name'] == list(result.keys())[0], 'result'] = \"\"\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "def calculate_NMI(df):\n",
    "    \n",
    "      ground_truth = df['label']\n",
    "      predictions = df['result']\n",
    "      \n",
    "      # Calculate Normalized Mutual Information Score\n",
    "      nmi_score = normalized_mutual_info_score(ground_truth, predictions)\n",
    "      print(f\"Normalized Mutual Information Score: {nmi_score}\")\n",
    "      return nmi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def calculate_purity(df):\n",
    "    \n",
    "      predicted_labels = np.array(df['result'])\n",
    "      true_labels = np.array(df['label'])\n",
    "      \n",
    "      # Get unique clusters\n",
    "      unique_clusters = np.unique(predicted_labels)\n",
    "      \n",
    "      # Total number of instances\n",
    "      total_instances = len(true_labels)\n",
    "      \n",
    "      # Calculate the number of correctly classified instances in each cluster\n",
    "      correctly_classified = 0\n",
    "      for cluster in unique_clusters:\n",
    "            # Get the true labels of instances in the current cluster\n",
    "            indices_in_cluster = np.where(predicted_labels == cluster)[0]\n",
    "            labels_in_cluster = true_labels[indices_in_cluster]\n",
    "            \n",
    "            # Determine the most common true label in this cluster\n",
    "            majority_label_count = Counter(labels_in_cluster).most_common(1)[0][1]\n",
    "            \n",
    "            # Add the number of correctly classified instances in this cluster\n",
    "            correctly_classified += majority_label_count\n",
    "      \n",
    "      # Calculate purity\n",
    "      purity = correctly_classified / total_instances\n",
    "      print(f\"Purity: {purity}\")\n",
    "      return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information Score: 0.6438803485292871\n",
      "0.6438803485292871\n",
      "Purity: 0.7726763717805151\n",
      "0.7726763717805151\n"
     ]
    }
   ],
   "source": [
    "print(calculate_NMI(df))\n",
    "print(calculate_purity(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
